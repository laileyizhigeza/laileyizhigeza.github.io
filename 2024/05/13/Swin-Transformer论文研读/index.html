<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本篇博客主要分享鸽砸对Swin-Transformer网络的学习笔记，内容摘自b站博主霹雳吧啦Wz，更详细的内容推荐大家去看博主的视频与CSDN博客。 Swin-Transformer论文：Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 代码地址：Swin-Transformer-github 视频地址">
<meta property="og:type" content="article">
<meta property="og:title" content="Swin-Transformer论文研读">
<meta property="og:url" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/index.html">
<meta property="og:site_name" content="来了一只鸽砸的书房">
<meta property="og:description" content="本篇博客主要分享鸽砸对Swin-Transformer网络的学习笔记，内容摘自b站博主霹雳吧啦Wz，更详细的内容推荐大家去看博主的视频与CSDN博客。 Swin-Transformer论文：Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 代码地址：Swin-Transformer-github 视频地址">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/architecture.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/ppln.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Pm.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SWB.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/MB.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/Window.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SW.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/shift1.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/shift2.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/shift3.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/mask.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/rpb.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/e.png">
<meta property="og:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/e2.png">
<meta property="article:published_time" content="2024-05-13T09:38:07.000Z">
<meta property="article:modified_time" content="2024-05-14T13:25:18.823Z">
<meta property="article:author" content="鸽砸">
<meta property="article:tag" content="神经网络模块">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/architecture.png">

<link rel="canonical" href="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Swin-Transformer论文研读 | 来了一只鸽砸的书房</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/rss2.xml" title="来了一只鸽砸的书房" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/laileyizhigeza" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">来了一只鸽砸的书房</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="鸽砸">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="来了一只鸽砸的书房">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Swin-Transformer论文研读
        </h1>

        <div class="post-meta">
        
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-05-13 17:38:07" itemprop="dateCreated datePublished" datetime="2024-05-13T17:38:07+08:00">2024-05-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-14 21:25:18" itemprop="dateModified" datetime="2024-05-14T21:25:18+08:00">2024-05-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本篇博客主要分享鸽砸对Swin-Transformer网络的学习笔记，内容摘自b站博主<a target="_blank" rel="noopener" href="https://space.bilibili.com/18161609?spm_id_from=333.337.0.0">霹雳吧啦Wz</a>，更详细的内容推荐大家去看博主的视频与CSDN博客。</p>
<p>Swin-Transformer论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.14030">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p>
<p>代码地址：<a target="_blank" rel="noopener" href="https://github.com/microsoft/Swin-Transformer">Swin-Transformer-github</a></p>
<p>视频地址：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pL4y1v7jC/?spm_id_from=333.999.0.0&vd_source=aa9dd7988d117d83ddfee471a4e9896b">Swin-Transformer网络结构详解</a></p>
 <span id="more"></span>

<h1 id="Swin-Transformer网络结构"><a href="#Swin-Transformer网络结构" class="headerlink" title="Swin-Transformer网络结构"></a>Swin-Transformer网络结构</h1><p>&ensp;&ensp;&ensp;&ensp;Swin-Transformer网络结构如下图所示。从图可以看出，Swin-Transformer主要由这几个模块组成：<strong>Patch Partition</strong>、<strong>Linear Embedding</strong>、<strong>Patch Merging</strong>、<strong>Swin Transformer Block</strong>。</p>
<p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/architecture.png"></p>
<p>&ensp;&ensp;&ensp;&ensp;上图展示的是标准的Swin-Transformer，图像再经过网络后，会被下采样32倍，通道数变为4C，可以在后面添加一个全局平均池化和全连接头进行分类任务；也可以融合不同层的输出，进行目标检测、实例分割等任务。</p>
<h2 id="Patch-Partition、Linear-Embedding"><a href="#Patch-Partition、Linear-Embedding" class="headerlink" title="Patch Partition、Linear Embedding"></a>Patch Partition、Linear Embedding</h2><p>&ensp;&ensp;&ensp;&ensp;在代码中，Patch Partition和Linear Embedding步骤是在一个类中实现的，因此我们放在一起讲。它们的处理流程如下图所示。在Patch Partition阶段，将图像按照4x4的大小切分为图像块，在通道方向上将图像块进行拼接。以HxWx3的输入图像为例，经过Patch Partition后图像大小变为H&#x2F;4xW&#x2F;4x48。</p>
<p>&ensp;&ensp;&ensp;&ensp;在Linear Embedding阶段，经过LayerNorm和线性层，将图像的通道数变为C。</p>
<p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/ppln.png"></p>
<p>&ensp;&ensp;&ensp;&ensp;在代码中，Patch Partition和Linear Embedding融合在了一起。使用卷积核大小4x4、步长为4、输出通道数为96的卷积层和LayerNorm层即实现了对图像的处理。部分代码如下所示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class PatchEmbed(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    2D Image to Patch Embedding</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, patch_size=4, in_c=3, embed_dim=96, norm_layer=None):</span><br><span class="line">        super().__init__()</span><br><span class="line">        patch_size = (patch_size, patch_size)</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.in_chans = in_c</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        # 这里的下采样实际上通过卷积层实现</span><br><span class="line">        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line"></span><br><span class="line">        # padding</span><br><span class="line">        # 如果输入图片的H，W不是patch_size的整数倍，需要进行padding</span><br><span class="line">        pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)</span><br><span class="line">        if pad_input:</span><br><span class="line">            # to pad the last 3 dimensions,</span><br><span class="line">            # (W_left, W_right, H_top,H_bottom, C_front, C_back)</span><br><span class="line">            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1],</span><br><span class="line">                          0, self.patch_size[0] - H % self.patch_size[0],</span><br><span class="line">                          0, 0))</span><br><span class="line">        # 下采样patch_size倍</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line">        # flatten: [B, C, H, W] -&gt; [B, C, HW]</span><br><span class="line">        # transpose: [B, C, HW] -&gt; [B, HW, C]</span><br><span class="line">        x = x.flatten(2).transpose(1, 2)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        return x, H, W</span><br></pre></td></tr></table></figure>

<h2 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a>Patch Merging</h2><p>&ensp;&ensp;&ensp;&ensp;Patch Merging的作用是实现特征图的下采样。它的处理流程如下图所示。从下图可以看出，Patch Merging先将图像进行切片，分成了四个部分，将四个部分在通道方向上进行拼接。对于HxWxC的输入特征图，拼接后的图像大小为H&#x2F;2xW&#x2F;2x4C。随后再将特征图通过LayerNorm与线性层，将通道数从4C减小为2C。在思想上，Patch Merging与YOLOv5的<strong>Focus</strong>模块很类似。</p>
<img src="Pm.png" style="zoom:50%;" />

<p>&ensp;&ensp;&ensp;&ensp;在代码中，Patch Merging的实现方法如下图所示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class PatchMerging(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Patch Merging Layer.</span><br><span class="line">    Args:</span><br><span class="line">        dim (int): Number of input channels.</span><br><span class="line">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, dim, norm_layer=nn.LayerNorm):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)</span><br><span class="line">        self.norm = norm_layer(4 * dim)</span><br><span class="line"></span><br><span class="line">    def forward(self, x, H, W):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        x: B, H*W, C</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        assert L == H * W, &quot;input feature has wrong size&quot;</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line">        # padding</span><br><span class="line">        # 如果输入feature map的H，W不是2的整数倍，需要进行padding</span><br><span class="line">        pad_input = (H % 2 == 1) or (W % 2 == 1)</span><br><span class="line">        if pad_input:</span><br><span class="line">            # to pad the last 3 dimensions, starting from the last dimension and moving forward.</span><br><span class="line">            # (C_front, C_back, W_left, W_right, H_top, H_bottom)</span><br><span class="line">            # 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同</span><br><span class="line">            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))</span><br><span class="line">        x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]</span><br><span class="line">        x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]</span><br><span class="line">        x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]</span><br><span class="line">        x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]</span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -1)  # [B, H/2, W/2, 4*C]</span><br><span class="line">        x = x.view(B, -1, 4 * C)  # [B, H/2*W/2, 4*C]</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)  # [B, H/2*W/2, 2*C]</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<h2 id="Swin-Transformer-Block"><a href="#Swin-Transformer-Block" class="headerlink" title="Swin Transformer Block"></a>Swin Transformer Block</h2><p>&ensp;&ensp;&ensp;&ensp;Swin Transformer Block是Swin Transformer的灵魂，它的网络结构如下图所示。下图实际上是两个Swin Transformer Block。</p>
<img src="SWB.png" style="zoom:67%;" />

<p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，特征图进入到Swin Transformer Block后，首先经过一个LayerNorm层，然后根据情况进入W-MSA&#x2F;SW-MSA模块，随后经过LayerNorm和MLP模块，其中存在网络的跳连接。我们在下面分别介绍各个模块的内容。</p>
<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><p>&ensp;&ensp;&ensp;&ensp;MLP模块的网络结构如下图所示。从下图可以看出，图像输入到MLP模块后，首先经过线性层，将通道数变为原来的4倍，然后依次经过GELU激活函数与Dropout，第二个线性层将通道数恢复到输入通道数，经过Dropout层后输出。</p>
<img src="MB.png" style="zoom:80%;" />

<p>&ensp;&ensp;&ensp;&ensp;MLP模块的实现代码如下所示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Mlp(nn.Module):</span><br><span class="line">    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):</span><br><span class="line">        super().__init__()</span><br><span class="line">        out_features = out_features or in_features</span><br><span class="line">        hidden_features = hidden_features or in_features</span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.drop1 = nn.Dropout(drop)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop2 = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<h3 id="W-MSA"><a href="#W-MSA" class="headerlink" title="W-MSA"></a>W-MSA</h3><p>&ensp;&ensp;&ensp;&ensp;在Swin Transformer中，执行注意力机制时并不是图像中的所有像素（Patch）都要进行注意力的计算，作者将图像划分为一个个小的窗口（window），然后窗口内的像素进行注意力机制计算，而窗口之间是没有信息交互的。如下图所示，图像按照2x2的window大小被切分成了4个窗口，随后4个窗口内部进行独立的注意力机制计算。这么做的好处是可以大大减小参数量与计算量。</p>
<img src="Window.png" style="zoom:50%;" />

<p>&ensp;&ensp;&ensp;&ensp;正如W-MSA的全程window-multiple self attention，模型以窗口为单位，进行注意力的计算，但这又会引出一个问题，那就是窗口之间没有信息的交互，模型的效果不好，因此，在Swin Transformer Block中，又引入了SW-MSA模块。</p>
<h3 id="SW-MSA"><a href="#SW-MSA" class="headerlink" title="SW-MSA"></a>SW-MSA</h3><p>&ensp;&ensp;&ensp;&ensp;SW-MSA全称shift window-multiple self attention，在W-MSA模块的基础上引入了窗口转移机制，如下图所示。Layer i表示的是W-MSA模块处理阶段，图像被切分成了4个window；Layer i+1表示SW-MSA模块处理阶段，可以看出，窗口的大小、数量都发生了变化，通过SW-MSA模块，实现了不同窗口之间的信息交互。</p>
<img src="SW.png" style="zoom: 67%;" />

<p>&ensp;&ensp;&ensp;&ensp;但我们从上图可以看出，SW-MSA的窗口大小不是一致的，这样会导致模型无法并行化进行，因为不同窗口的注意力计算方式不同，为了解决这个问题，作者提出了一种转换方法，通过调整，可以使SW-MSA模块能够像W-MSA模块一样在固定大小的窗口中计算注意力。转换方法如下所示。首先对切分的窗口进行编号，然后将图像中的一些窗口进行编号，从下图可看出窗口的组合被标记为了A、B、C。</p>
<img src="shift1.png" style="zoom:50%;" />

<p>&ensp;&ensp;&ensp;&ensp;随后将A、C组合移动到图像的最下方。</p>
<img src="shift2.png" style="zoom:50%;" />

<p>&ensp;&ensp;&ensp;&ensp;最后将A、B模块移动到图像的最右侧，这样就完成了转换，在计算注意力时，编号4可以独自进行计算；编号5、3组合进行计算；编号7、1组合进行计算；编号8、6、2、0组合进行计算，这样就实现了在SW-MSA模块中保持各窗口大小一致，可进行注意力计算了。</p>
<img src="shift3.png" style="zoom:50%;" />

<p>&ensp;&ensp;&ensp;&ensp;但是，我们可以发现一个问题，以编号5、3的组合窗口为例，实际上窗口5和窗口3之间不是连续的，计算它们之间的注意力没有任何价值，我们希望的是窗口5内部进行注意力计算、窗口3内部进行注意力计算。因此，作者提出了一种mask矩阵，用来去除那些没有意义的注意力计算权重，方法如下图所示。</p>
<p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/mask.png"></p>
<p>&ensp;&ensp;&ensp;&ensp;以区域5和区域3组成的窗口为例，作者将没有价值的权重，如a02、a03减去一个常数100，这样在计算softmax权重时，它们的结果就会是0，这就可以实现窗口内部的注意力机制计算了。</p>
<h2 id="其他技巧"><a href="#其他技巧" class="headerlink" title="其他技巧"></a>其他技巧</h2><h3 id="Relative-Position-Bias"><a href="#Relative-Position-Bias" class="headerlink" title="Relative Position Bias"></a>Relative Position Bias</h3><p>&ensp;&ensp;&ensp;&ensp;在进行多头注意力计算时，作者在Q、K相乘之后，加上了一个Relative Position Bias，如下图所示。<strong>B</strong>表示的就是相对位置偏置，这个偏置的值是通过网络学习得到的，作者将其称之为Relative Position Bias Table，它的大小为（2M-1）x（2M-1），M表示的是图像的长&#x2F;宽大小。</p>
<p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/rpb.png"></p>
<p>&ensp;&ensp;&ensp;&ensp;每个像素（patch）对应的Relative Position Bias值需要通过自己的Relative Position Index得到，Index的计算方法通过图像的绝对位置和相对位置综合计算得到。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="对比试验"><a href="#对比试验" class="headerlink" title="对比试验"></a>对比试验</h2><p>&ensp;&ensp;&ensp;&ensp;我们最后看看作者做的实验~下图a是在ImageNet-1K数据集上对模型进行训练，然后查看模型的分类性能。可以发现Swin-B（Base）的性能优于其他模型。图b是在ImageNet-22K数据集上对模型进行训练，可以发现Swin-B的性能也是最好的。</p>
<img src="e.png" style="zoom:67%;" />

<h2 id="移动窗口与相对位置编码实验"><a href="#移动窗口与相对位置编码实验" class="headerlink" title="移动窗口与相对位置编码实验"></a>移动窗口与相对位置编码实验</h2><p>&ensp;&ensp;&ensp;&ensp;作者比较了在注意力计算中添加不同形式的位置编码，如下图所示。可以发现，添加移动窗口，即SW-MSA时，效果最好；在注意力计算中加入相对位置编码（relative positon bias）时，效果最好。</p>
<p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/e2.png"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&ensp;&ensp;&ensp;&ensp;Swin-Transformer在CV领域的地位很高，很有学习价值！</p>

    </div>

    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      </div>
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>鸽砸
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" title="Swin-Transformer论文研读">http://example.com/2024/05/13/Swin-Transformer论文研读/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97/" rel="tag"># 神经网络模块</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/04/17/BatchNorm%E5%92%8CLayerNorm%E7%9A%84%E7%90%86%E8%A7%A3/" rel="prev" title="BatchNorm和LayerNorm的理解">
      <i class="fa fa-chevron-left"></i> BatchNorm和LayerNorm的理解
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/" rel="next" title="ConvNeXt论文研读">
      ConvNeXt论文研读 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Swin-Transformer%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">Swin-Transformer网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Patch-Partition%E3%80%81Linear-Embedding"><span class="nav-number">1.1.</span> <span class="nav-text">Patch Partition、Linear Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Patch-Merging"><span class="nav-number">1.2.</span> <span class="nav-text">Patch Merging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Swin-Transformer-Block"><span class="nav-number">1.3.</span> <span class="nav-text">Swin Transformer Block</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MLP"><span class="nav-number">1.3.1.</span> <span class="nav-text">MLP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#W-MSA"><span class="nav-number">1.3.2.</span> <span class="nav-text">W-MSA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SW-MSA"><span class="nav-number">1.3.3.</span> <span class="nav-text">SW-MSA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%8A%80%E5%B7%A7"><span class="nav-number">1.4.</span> <span class="nav-text">其他技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Relative-Position-Bias"><span class="nav-number">1.4.1.</span> <span class="nav-text">Relative Position Bias</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">2.</span> <span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E8%AF%95%E9%AA%8C"><span class="nav-number">2.1.</span> <span class="nav-text">对比试验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A7%BB%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%8E%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%AE%9E%E9%AA%8C"><span class="nav-number">2.2.</span> <span class="nav-text">移动窗口与相对位置编码实验</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="鸽砸"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">鸽砸</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/laileyizhigeza" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;laileyizhigeza" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xlh15534418471@163.com" title="E-Mail → mailto:xlh15534418471@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://pan.laileyizhigeza.cn/" title="YunCloud → http:&#x2F;&#x2F;pan.laileyizhigeza.cn" rel="noopener" target="_blank"><i class="fa fa-trash fa-fw"></i>YunCloud</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://github.com/laileyizhigeza" title="https:&#x2F;&#x2F;github.com&#x2F;laileyizhigeza" rel="noopener" target="_blank">鸽砸的github</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2024-04 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">鸽砸</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
