<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>来了一只鸽砸的书房</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Thu, 16 May 2024 08:44:38 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>ConvNeXt论文研读</title>
      <link>http://example.com/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</link>
      <guid>http://example.com/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</guid>
      <pubDate>Tue, 14 May 2024 12:37:03 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;本篇博客是对ConvNeXt论文的学习笔记，内容摘自b站博主&lt;a href=&quot;https://space.bilibili.com/18161609?spm_id_from=333.337.0.0&quot;&gt;霹雳吧啦Wz&lt;/a&gt;，更详细的内容推荐大家去看博主的视频与CSDN博客。（</description>
        
      
      
      
      <content:encoded><![CDATA[<p>本篇博客是对ConvNeXt论文的学习笔记，内容摘自b站博主<a href="https://space.bilibili.com/18161609?spm_id_from=333.337.0.0">霹雳吧啦Wz</a>，更详细的内容推荐大家去看博主的视频与CSDN博客。（逮着一只羊毛薅~）</p><p>Swin-Transformer论文：<a href="https://arxiv.org/abs/2201.03545">A ConvNet for the 2020s</a></p><p>代码地址：<a href="https://github.com/facebookresearch/ConvNeXt">ConvNeXt-github</a></p><p>视频地址：<a href="https://www.bilibili.com/video/BV1SS4y157fu/?spm_id_from=333.1007.top_right_bar_window_history.content.click">ConvNeXt网络讲解</a></p><h1 id="ConvNeXt技巧"><a href="#ConvNeXt技巧" class="headerlink" title="ConvNeXt技巧"></a>ConvNeXt技巧</h1><p>&ensp;&ensp;&ensp;&ensp;ConvNeXt论文是作者向Swin-Transformer网络靠齐的过程，基于卷积网络设计，但在模型结构、训练技巧上与Swin-Transformer十分类似。总的来说，ConvNeXt基于ResNet进行设计，我们在下面分别介绍论文中提出的各种trick~从下图可以看出，原始的ResNet50在数据集上的准确率为78.8%。</p><img src="side.png" style="zoom:67%;" /><h2 id="Marco-Design"><a href="#Marco-Design" class="headerlink" title="Marco Design"></a>Marco Design</h2><p>&ensp;&ensp;&ensp;&ensp;Marco Design介绍的是作者在网络总体结构上做出的一些变化。</p><h3 id="stage-ratio"><a href="#stage-ratio" class="headerlink" title="stage ratio"></a>stage ratio</h3><p>&ensp;&ensp;&ensp;&ensp;我们首先看看ResNet的网络组成。以ResNet50为例，我们可以看到网络模块的比例是3：4：6：3，其中每一层的特征图大小不会改变，层与层之间进行特征图的下采样处理。</p><img src="resblock.png" style="zoom: 50%;" /><p>&ensp;&ensp;&ensp;&ensp;我们再看看Swin-Transformer的网络结构，可以看出，网络模块的比例为2：2：6：2，归一化后是1：1：3：1。</p><img src="architecture.png" style="zoom: 80%;" /><p>&ensp;&ensp;&ensp;&ensp;因此，作者根据Swin-Transformer的网络结构比例，修改了自己网络的比例，将模块的比例变成了<strong>3：3：9：3</strong>。通过这样调整，模型的准确率提高到了79.4%。</p><h3 id="“Patchify-stem”"><a href="#“Patchify-stem”" class="headerlink" title="“Patchify stem”"></a>“Patchify stem”</h3><p>&ensp;&ensp;&ensp;&ensp;这里的Patchify stem指的是网络最初下采样的方式，在原始的ResNet中，使用的是卷积核大小7x7，步长为2，输出通道数为64的卷积核和3x3大小、步长为2的最大池化层实现的，将输入特征图变为原来的1&#x2F;4。但在Swin-Transformer中，最初的下采样通过Patch Partition+Linear Embedding实现，具体方法是采用卷积核大小4x4、步长为4的卷积层实现。作者借鉴了Swin-Transformer的这种下采样方式，通过这种改进，将准确率提高到79.5%。</p><h2 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h2><h3 id="depth-conv"><a href="#depth-conv" class="headerlink" title="depth conv"></a>depth conv</h3><p>&ensp;&ensp;&ensp;&ensp;原始的ResNetBlock结构如下图所示。使用到了三个卷积块与一个跳连接组成，其中两端为1x1卷积，中间为3x3卷积。</p><p><img src="/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/RB.png"></p><p>&ensp;&ensp;&ensp;&ensp;而在ResNeXt中，将3x3卷积替换成了组卷积，组卷积就是对特征图的每一个通道都只是用一个卷积核进行处理，可以大大减小模型的参数，在ResNeXt论文中，作者认为这种组卷积结构可以平衡FLOPS和Accuracy，如下图所示。下图可以看出，将图像分为了256&#x2F;4&#x3D;64组。</p><p><img src="/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/RDB.png"></p><p>&ensp;&ensp;&ensp;&ensp;在本篇论文中，将所有ResNetBlock中间的3x3卷积都替换为了组卷积，并将组数设置为了通道数。通过这个操作，准确率降低为78.3，但运算量也降低到了2.4GFLOPS。</p><h3 id="increasing-width"><a href="#increasing-width" class="headerlink" title="increasing width"></a>increasing width</h3><p>&ensp;&ensp;&ensp;&ensp;相比ResNet，Swin-Transformer中各模块输出的通道数都较高，如下图所示。比如以ResNet50与Swin-B之间进行比较，ResNet在第一次下采样输出的通道数为64，而Swin-B第一次输出的通道数为96。</p><img src="concate.png" style="zoom: 80%;" /><p>&ensp;&ensp;&ensp;&ensp;因此，作者扩大ResNet的输出通道数，让其与Swin-Transformer的输出通道数一致，通过这种方法，模型的准确率提升到了80.5%。</p><h3 id="inverting-dims"><a href="#inverting-dims" class="headerlink" title="inverting dims"></a>inverting dims</h3><p>&ensp;&ensp;&ensp;&ensp;在Swin-Transformer中，MLP模块的网络结构如下图所示。从下图可以看出，MLP先使用Linear将输入特征图的通道数变为原来的4倍，然后最后的Linear再将特征图复原为原始通道数。</p><img src="MB.png" style="zoom:67%;" /><p>&ensp;&ensp;&ensp;&ensp;作者认为，MLP模块非常想MobileNetV2中的Inverted Bottleneck模块，即两头粗，中间细，作者将这种结果用在了ConvNeXt的结构中，如下图所示。可以看出，修改后的模块结构如图b所示。</p><p><img src="/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/IB.png"></p><p>&ensp;&ensp;&ensp;&ensp;通过上图这种修改方式，在较小的模型上准确率由80.5%提升到了80.6%；在较大的模型上准确率由81.9%提升到了82.6%。</p><h2 id="Large-Kernel"><a href="#Large-Kernel" class="headerlink" title="Large Kernel"></a>Large Kernel</h2><h3 id="move-up-D-Conv"><a href="#move-up-D-Conv" class="headerlink" title="move up D.Conv"></a>move up D.Conv</h3><p>&ensp;&ensp;&ensp;&ensp;在Swin-Transformer Block中，MSA模块被放在了MLP层之前，作者认为在ResNetBlock中的分组卷积在作用上与MSA模块类似，因此将分组卷积移动到了最上面，同时保持图b那种中间粗、两头细的网络结构，如下图所示。</p><p><img src="/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/md.png"></p><p>&ensp;&ensp;&ensp;&ensp;通过上述调整，模型的准确率从80.6%降低到了79.9%，但是参数量从4.6%减小到了4.1%。</p><h3 id="increasing-kernel-size"><a href="#increasing-kernel-size" class="headerlink" title="increasing kernel size"></a>increasing kernel size</h3><p>&ensp;&ensp;&ensp;&ensp;作者对分层卷积的卷积核进行调整，通过扩大<strong>分层卷积卷积核</strong>的大小，并观察结果，如下图所示。</p><p><img src="/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/lk.png"></p><p>&ensp;&ensp;&ensp;&ensp;从下图可以看出，当卷积核大小扩大到<strong>7x7</strong>时，在数据集的准确率与网络参数可以达到一个较好的平衡，当卷积核大小进一步扩大时，模型的效果反而下降了。通过上述改进，模型的准确率由79.9%提升到了80.6%。</p><h2 id="Micro-Design"><a href="#Micro-Design" class="headerlink" title="Micro Design"></a>Micro Design</h2><p>&ensp;&ensp;&ensp;&ensp;这里主要介绍一些细节模块的调整与改进。</p><img src="Micro Disgn.png" style="zoom:67%;" /><h3 id="ReLU—-GeLU"><a href="#ReLU—-GeLU" class="headerlink" title="ReLU—&gt;GeLU"></a>ReLU—&gt;GeLU</h3><p>&ensp;&ensp;&ensp;&ensp;在传统图像领域，深度网络通常都是用ReLU激活函数，而在Swin-Transformer中，使用到了GeLU激活函数，作者将这种激活函数的改变应用到了ConNeXt中，将ReLU激活函数替换为ReLU激活函数，通过这种调整，网络的性能保持80.6%不变。</p><h3 id="fewer-activations"><a href="#fewer-activations" class="headerlink" title="fewer activations"></a>fewer activations</h3><p>&ensp;&ensp;&ensp;&ensp;在ResNetBlock中，每一个卷积层之后都接了激活函数，而在Swin-Transformer Block中，激活函数的数量较少，因此，作者在ConvNeXtBlock中使用到了更少的激活函数。通过这种调整，模型的准确率由80.6%提升到了81.3%。</p><h3 id="fewer-norms"><a href="#fewer-norms" class="headerlink" title="fewer norms"></a>fewer norms</h3><p>&ensp;&ensp;&ensp;&ensp;与激活函数一样，作者减少了归一化层的数量，原本的ResNetBlock中，每个卷积层后都接上了Batch Normalization，在Swin-TransformerBlock中，LN的数量较少。作者根据Swin-TransformerBlock的结构，减少了ConvNeXtBlock中Norm的数量。通过这种调整，模型的准确率由81.4%提升到了81.5%。</p><h3 id="separate-downsampling-layers"><a href="#separate-downsampling-layers" class="headerlink" title="separate downsampling layers"></a>separate downsampling layers</h3><p>&ensp;&ensp;&ensp;&ensp;在ResNetBlock中，作者进行下采样的方式如下图所示。可以看出，在网络正常分支，使用卷积核大小3x3、步长为2的卷积进行下采样；在分支结构中，使用卷积核大小1x1，步长为2的卷积进行下采样，最后将这两个分支的结果进行融合，实现特征图的下采样。</p><img src="dso.png" style="zoom:80%;" /><p>&ensp;&ensp;&ensp;&ensp;在Swin-TransformerBlock中，使用了Patch Merging方式进行下采样，如下图所示。</p><img src="PM.png" style="zoom:80%;" /><p>&ensp;&ensp;&ensp;&ensp;依据Swin-TransformerBlock中的下采样方式，ConvNeXt使用卷积核大小2x2，步长为2的卷积，和LN层进行特征图下采样，如下图所示。</p><p><img src="/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/DS.png"></p><p>&ensp;&ensp;&ensp;&ensp;通过上述调整，模型的准确率从81.5%提高到了82.0%，高于Swin-Transformer的81.3%。</p><h1 id="ConvNeXt网络结构"><a href="#ConvNeXt网络结构" class="headerlink" title="ConvNeXt网络结构"></a>ConvNeXt网络结构</h1><p>&ensp;&ensp;&ensp;&ensp;通过上述提到的一系列技巧，我们最后看看ConvNeXt-T的网络结构。</p><img src="ConvNeXt.png" style="zoom:80%;" /><p>&ensp;&ensp;&ensp;&ensp;在ConvNeXtBlock中，我们可以观察到模块最后有一个Layer Scale的结构，这个结构其实类似于SeNet的思想，不过比SeNet更加简单，对于HxWxC的特征图，Layer Scale阶段会生成一组1x1xC的可学习向量，在Layer Scale阶段直接将1x1xC的向量与HxWxC的向量相乘即可。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>&ensp;&ensp;&ensp;&ensp;我们最后简单看看ConvNeXt的相关实验。下图可以看出，无论是在ImageNet-1K还是ImageNet-22K数据集上进行训练，ConvNeXt的效果都是最好的。</p><img src="e1.png" style="zoom:80%;" /><p>&ensp;&ensp;&ensp;&ensp;下图表明的是ConvNeXt的超参数配置。</p><img src="e2.png" style="zoom:80%;" /><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&ensp;&ensp;&ensp;&ensp;ConvNeXt虽然没有提出十分有创新性的网络结构、训练方法，但通过大量的实验，证明了卷积还存在的巨大的潜力，等待着我们的探索~</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      <category domain="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97/">神经网络模块</category>
      
      
      <comments>http://example.com/2024/05/14/ConvNeXt%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Swin-Transformer论文研读</title>
      <link>http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</link>
      <guid>http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</guid>
      <pubDate>Mon, 13 May 2024 09:38:07 GMT</pubDate>
      
      <description>&lt;p&gt;本篇博客主要分享鸽砸对Swin-Transformer网络的学习笔记，内容摘自b站博主&lt;a href=&quot;https://space.bilibili.com/18161609?spm_id_from=333.337.0.0&quot;&gt;霹雳吧啦Wz&lt;/a&gt;，更详细的内容推荐大家去看博主的视频与CSDN博客。&lt;/p&gt;
&lt;p&gt;Swin-Transformer论文：&lt;a href=&quot;https://arxiv.org/abs/2103.14030&quot;&gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码地址：&lt;a href=&quot;https://github.com/microsoft/Swin-Transformer&quot;&gt;Swin-Transformer-github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;视频地址：&lt;a href=&quot;https://www.bilibili.com/video/BV1pL4y1v7jC/?spm_id_from=333.999.0.0&amp;vd_source=aa9dd7988d117d83ddfee471a4e9896b&quot;&gt;Swin-Transformer网络结构详解&lt;/a&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>本篇博客主要分享鸽砸对Swin-Transformer网络的学习笔记，内容摘自b站博主<a href="https://space.bilibili.com/18161609?spm_id_from=333.337.0.0">霹雳吧啦Wz</a>，更详细的内容推荐大家去看博主的视频与CSDN博客。</p><p>Swin-Transformer论文：<a href="https://arxiv.org/abs/2103.14030">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p><p>代码地址：<a href="https://github.com/microsoft/Swin-Transformer">Swin-Transformer-github</a></p><p>视频地址：<a href="https://www.bilibili.com/video/BV1pL4y1v7jC/?spm_id_from=333.999.0.0&vd_source=aa9dd7988d117d83ddfee471a4e9896b">Swin-Transformer网络结构详解</a></p> <span id="more"></span><h1 id="Swin-Transformer网络结构"><a href="#Swin-Transformer网络结构" class="headerlink" title="Swin-Transformer网络结构"></a>Swin-Transformer网络结构</h1><p>&ensp;&ensp;&ensp;&ensp;Swin-Transformer网络结构如下图所示。从图可以看出，Swin-Transformer主要由这几个模块组成：<strong>Patch Partition</strong>、<strong>Linear Embedding</strong>、<strong>Patch Merging</strong>、<strong>Swin Transformer Block</strong>。</p><p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/architecture.png"></p><p>&ensp;&ensp;&ensp;&ensp;上图展示的是标准的Swin-Transformer，图像再经过网络后，会被下采样32倍，通道数变为4C，可以在后面添加一个全局平均池化和全连接头进行分类任务；也可以融合不同层的输出，进行目标检测、实例分割等任务。</p><h2 id="Patch-Partition、Linear-Embedding"><a href="#Patch-Partition、Linear-Embedding" class="headerlink" title="Patch Partition、Linear Embedding"></a>Patch Partition、Linear Embedding</h2><p>&ensp;&ensp;&ensp;&ensp;在代码中，Patch Partition和Linear Embedding步骤是在一个类中实现的，因此我们放在一起讲。它们的处理流程如下图所示。在Patch Partition阶段，将图像按照4x4的大小切分为图像块，在通道方向上将图像块进行拼接。以HxWx3的输入图像为例，经过Patch Partition后图像大小变为H&#x2F;4xW&#x2F;4x48。</p><p>&ensp;&ensp;&ensp;&ensp;在Linear Embedding阶段，经过LayerNorm和线性层，将图像的通道数变为C。</p><p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/ppln.png"></p><p>&ensp;&ensp;&ensp;&ensp;在代码中，Patch Partition和Linear Embedding融合在了一起。使用卷积核大小4x4、步长为4、输出通道数为96的卷积层和LayerNorm层即实现了对图像的处理。部分代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class PatchEmbed(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    2D Image to Patch Embedding</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, patch_size=4, in_c=3, embed_dim=96, norm_layer=None):</span><br><span class="line">        super().__init__()</span><br><span class="line">        patch_size = (patch_size, patch_size)</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.in_chans = in_c</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        # 这里的下采样实际上通过卷积层实现</span><br><span class="line">        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line"></span><br><span class="line">        # padding</span><br><span class="line">        # 如果输入图片的H，W不是patch_size的整数倍，需要进行padding</span><br><span class="line">        pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)</span><br><span class="line">        if pad_input:</span><br><span class="line">            # to pad the last 3 dimensions,</span><br><span class="line">            # (W_left, W_right, H_top,H_bottom, C_front, C_back)</span><br><span class="line">            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1],</span><br><span class="line">                          0, self.patch_size[0] - H % self.patch_size[0],</span><br><span class="line">                          0, 0))</span><br><span class="line">        # 下采样patch_size倍</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line">        # flatten: [B, C, H, W] -&gt; [B, C, HW]</span><br><span class="line">        # transpose: [B, C, HW] -&gt; [B, HW, C]</span><br><span class="line">        x = x.flatten(2).transpose(1, 2)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        return x, H, W</span><br></pre></td></tr></table></figure><h2 id="Patch-Merging"><a href="#Patch-Merging" class="headerlink" title="Patch Merging"></a>Patch Merging</h2><p>&ensp;&ensp;&ensp;&ensp;Patch Merging的作用是实现特征图的下采样。它的处理流程如下图所示。从下图可以看出，Patch Merging先将图像进行切片，分成了四个部分，将四个部分在通道方向上进行拼接。对于HxWxC的输入特征图，拼接后的图像大小为H&#x2F;2xW&#x2F;2x4C。随后再将特征图通过LayerNorm与线性层，将通道数从4C减小为2C。在思想上，Patch Merging与YOLOv5的<strong>Focus</strong>模块很类似。</p><img src="Pm.png" style="zoom:50%;" /><p>&ensp;&ensp;&ensp;&ensp;在代码中，Patch Merging的实现方法如下图所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">class PatchMerging(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Patch Merging Layer.</span><br><span class="line">    Args:</span><br><span class="line">        dim (int): Number of input channels.</span><br><span class="line">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, dim, norm_layer=nn.LayerNorm):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)</span><br><span class="line">        self.norm = norm_layer(4 * dim)</span><br><span class="line"></span><br><span class="line">    def forward(self, x, H, W):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        x: B, H*W, C</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        assert L == H * W, &quot;input feature has wrong size&quot;</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line">        # padding</span><br><span class="line">        # 如果输入feature map的H，W不是2的整数倍，需要进行padding</span><br><span class="line">        pad_input = (H % 2 == 1) or (W % 2 == 1)</span><br><span class="line">        if pad_input:</span><br><span class="line">            # to pad the last 3 dimensions, starting from the last dimension and moving forward.</span><br><span class="line">            # (C_front, C_back, W_left, W_right, H_top, H_bottom)</span><br><span class="line">            # 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同</span><br><span class="line">            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))</span><br><span class="line">        x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]</span><br><span class="line">        x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]</span><br><span class="line">        x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]</span><br><span class="line">        x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]</span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -1)  # [B, H/2, W/2, 4*C]</span><br><span class="line">        x = x.view(B, -1, 4 * C)  # [B, H/2*W/2, 4*C]</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)  # [B, H/2*W/2, 2*C]</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h2 id="Swin-Transformer-Block"><a href="#Swin-Transformer-Block" class="headerlink" title="Swin Transformer Block"></a>Swin Transformer Block</h2><p>&ensp;&ensp;&ensp;&ensp;Swin Transformer Block是Swin Transformer的灵魂，它的网络结构如下图所示。下图实际上是两个Swin Transformer Block。</p><img src="SWB.png" style="zoom:67%;" /><p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，特征图进入到Swin Transformer Block后，首先经过一个LayerNorm层，然后根据情况进入W-MSA&#x2F;SW-MSA模块，随后经过LayerNorm和MLP模块，其中存在网络的跳连接。我们在下面分别介绍各个模块的内容。</p><h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><p>&ensp;&ensp;&ensp;&ensp;MLP模块的网络结构如下图所示。从下图可以看出，图像输入到MLP模块后，首先经过线性层，将通道数变为原来的4倍，然后依次经过GELU激活函数与Dropout，第二个线性层将通道数恢复到输入通道数，经过Dropout层后输出。</p><img src="MB.png" style="zoom:80%;" /><p>&ensp;&ensp;&ensp;&ensp;MLP模块的实现代码如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Mlp(nn.Module):</span><br><span class="line">    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):</span><br><span class="line">        super().__init__()</span><br><span class="line">        out_features = out_features or in_features</span><br><span class="line">        hidden_features = hidden_features or in_features</span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.drop1 = nn.Dropout(drop)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop2 = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h3 id="W-MSA"><a href="#W-MSA" class="headerlink" title="W-MSA"></a>W-MSA</h3><p>&ensp;&ensp;&ensp;&ensp;在Swin Transformer中，执行注意力机制时并不是图像中的所有像素（Patch）都要进行注意力的计算，作者将图像划分为一个个小的窗口（window），然后窗口内的像素进行注意力机制计算，而窗口之间是没有信息交互的。如下图所示，图像按照2x2的window大小被切分成了4个窗口，随后4个窗口内部进行独立的注意力机制计算。这么做的好处是可以大大减小参数量与计算量。</p><img src="Window.png" style="zoom:50%;" /><p>&ensp;&ensp;&ensp;&ensp;正如W-MSA的全程window-multiple self attention，模型以窗口为单位，进行注意力的计算，但这又会引出一个问题，那就是窗口之间没有信息的交互，模型的效果不好，因此，在Swin Transformer Block中，又引入了SW-MSA模块。</p><h3 id="SW-MSA"><a href="#SW-MSA" class="headerlink" title="SW-MSA"></a>SW-MSA</h3><p>&ensp;&ensp;&ensp;&ensp;SW-MSA全称shift window-multiple self attention，在W-MSA模块的基础上引入了窗口转移机制，如下图所示。Layer i表示的是W-MSA模块处理阶段，图像被切分成了4个window；Layer i+1表示SW-MSA模块处理阶段，可以看出，窗口的大小、数量都发生了变化，通过SW-MSA模块，实现了不同窗口之间的信息交互。</p><img src="SW.png" style="zoom: 67%;" /><p>&ensp;&ensp;&ensp;&ensp;但我们从上图可以看出，SW-MSA的窗口大小不是一致的，这样会导致模型无法并行化进行，因为不同窗口的注意力计算方式不同，为了解决这个问题，作者提出了一种转换方法，通过调整，可以使SW-MSA模块能够像W-MSA模块一样在固定大小的窗口中计算注意力。转换方法如下所示。首先对切分的窗口进行编号，然后将图像中的一些窗口进行编号，从下图可看出窗口的组合被标记为了A、B、C。</p><img src="shift1.png" style="zoom:50%;" /><p>&ensp;&ensp;&ensp;&ensp;随后将A、C组合移动到图像的最下方。</p><img src="shift2.png" style="zoom:50%;" /><p>&ensp;&ensp;&ensp;&ensp;最后将A、B模块移动到图像的最右侧，这样就完成了转换，在计算注意力时，编号4可以独自进行计算；编号5、3组合进行计算；编号7、1组合进行计算；编号8、6、2、0组合进行计算，这样就实现了在SW-MSA模块中保持各窗口大小一致，可进行注意力计算了。</p><img src="shift3.png" style="zoom:50%;" /><p>&ensp;&ensp;&ensp;&ensp;但是，我们可以发现一个问题，以编号5、3的组合窗口为例，实际上窗口5和窗口3之间不是连续的，计算它们之间的注意力没有任何价值，我们希望的是窗口5内部进行注意力计算、窗口3内部进行注意力计算。因此，作者提出了一种mask矩阵，用来去除那些没有意义的注意力计算权重，方法如下图所示。</p><p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/mask.png"></p><p>&ensp;&ensp;&ensp;&ensp;以区域5和区域3组成的窗口为例，作者将没有价值的权重，如a02、a03减去一个常数100，这样在计算softmax权重时，它们的结果就会是0，这就可以实现窗口内部的注意力机制计算了。</p><h2 id="其他技巧"><a href="#其他技巧" class="headerlink" title="其他技巧"></a>其他技巧</h2><h3 id="Relative-Position-Bias"><a href="#Relative-Position-Bias" class="headerlink" title="Relative Position Bias"></a>Relative Position Bias</h3><p>&ensp;&ensp;&ensp;&ensp;在进行多头注意力计算时，作者在Q、K相乘之后，加上了一个Relative Position Bias，如下图所示。<strong>B</strong>表示的就是相对位置偏置，这个偏置的值是通过网络学习得到的，作者将其称之为Relative Position Bias Table，它的大小为（2M-1）x（2M-1），M表示的是图像的长&#x2F;宽大小。</p><p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/rpb.png"></p><p>&ensp;&ensp;&ensp;&ensp;每个像素（patch）对应的Relative Position Bias值需要通过自己的Relative Position Index得到，Index的计算方法通过图像的绝对位置和相对位置综合计算得到。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="对比试验"><a href="#对比试验" class="headerlink" title="对比试验"></a>对比试验</h2><p>&ensp;&ensp;&ensp;&ensp;我们最后看看作者做的实验~下图a是在ImageNet-1K数据集上对模型进行训练，然后查看模型的分类性能。可以发现Swin-B（Base）的性能优于其他模型。图b是在ImageNet-22K数据集上对模型进行训练，可以发现Swin-B的性能也是最好的。</p><img src="e.png" style="zoom:67%;" /><h2 id="移动窗口与相对位置编码实验"><a href="#移动窗口与相对位置编码实验" class="headerlink" title="移动窗口与相对位置编码实验"></a>移动窗口与相对位置编码实验</h2><p>&ensp;&ensp;&ensp;&ensp;作者比较了在注意力计算中添加不同形式的位置编码，如下图所示。可以发现，添加移动窗口，即SW-MSA时，效果最好；在注意力计算中加入相对位置编码（relative positon bias）时，效果最好。</p><p><img src="/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/e2.png"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&ensp;&ensp;&ensp;&ensp;Swin-Transformer在CV领域的地位很高，很有学习价值！</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      <category domain="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97/">神经网络模块</category>
      
      
      <comments>http://example.com/2024/05/13/Swin-Transformer%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>BatchNorm和LayerNorm的理解</title>
      <link>http://example.com/2024/04/17/BatchNorm%E5%92%8CLayerNorm%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <guid>http://example.com/2024/04/17/BatchNorm%E5%92%8CLayerNorm%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <pubDate>Wed, 17 Apr 2024 10:34:36 GMT</pubDate>
      
      <description>&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;本篇文章是对知乎几篇关于BatchNorm和LayerNorm之间差异的一点点读后感，讲得很好！&lt;/p&gt;
&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;知乎原作者（大佬）：&lt;a href=&quot;https://www.zhihu.com/people/yan-xin-65&quot;&gt;严昕&lt;/a&gt;、&lt;a href=&quot;https://www.zhihu.com/people/june-lee-24/posts&quot;&gt;Steven&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;ensp;&amp;ensp;&amp;ensp;&amp;ensp;原文链接：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/656647661&quot;&gt;对比pytorch中的BatchNorm和LayerNorm层&lt;/a&gt;、&lt;a href=&quot;https://zhuanlan.zhihu.com/p/142866736&quot;&gt;深度学习中的标准化&lt;/a&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>&ensp;&ensp;&ensp;&ensp;本篇文章是对知乎几篇关于BatchNorm和LayerNorm之间差异的一点点读后感，讲得很好！</p><p>&ensp;&ensp;&ensp;&ensp;知乎原作者（大佬）：<a href="https://www.zhihu.com/people/yan-xin-65">严昕</a>、<a href="https://www.zhihu.com/people/june-lee-24/posts">Steven</a></p><p>&ensp;&ensp;&ensp;&ensp;原文链接：<a href="https://zhuanlan.zhihu.com/p/656647661">对比pytorch中的BatchNorm和LayerNorm层</a>、<a href="https://zhuanlan.zhihu.com/p/142866736">深度学习中的标准化</a></p> <span id="more"></span><h1 id="BatchNorm的步骤"><a href="#BatchNorm的步骤" class="headerlink" title="BatchNorm的步骤"></a>BatchNorm的步骤</h1><p>&ensp;&ensp;&ensp;&ensp;对于一个batch（batch_size&#x3D;m)下的输入数据<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="10.495ex" height="1.969ex" role="img" focusable="false" viewBox="0 -583 4638.9 870.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-1-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-1-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D707" xlink:href="#MJX-1-TEX-I-1D707"></use></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><use data-c="1D6FD" xlink:href="#MJX-1-TEX-I-1D6FD"></use></g></g><g data-mml-node="mo" transform="translate(1364,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(2419.8,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use><use data-c="2E" xlink:href="#MJX-1-TEX-N-2E" transform="translate(500,0)"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="2E" xlink:href="#MJX-1-TEX-N-2E"></use></g><g data-mml-node="mo" transform="translate(1056,0)"><use data-c="2E" xlink:href="#MJX-1-TEX-N-2E"></use></g><g data-mml-node="mi" transform="translate(1334,0)"><use data-c="1D45A" xlink:href="#MJX-1-TEX-I-1D45A"></use></g></g></g></g></g></svg></mjx-container>共m个数据，输出是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="11.646ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 5147.5 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-1-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(1094.7,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2150.5,0)"><use data-c="1D435" xlink:href="#MJX-1-TEX-I-1D435"></use></g><g data-mml-node="mi" transform="translate(2909.5,0)"><use data-c="1D441" xlink:href="#MJX-1-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(3797.5,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(4186.5,0)"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mo" transform="translate(4758.5,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container>，Batchnorm的步骤如下：</p><p>①求出此次批量数据x的均值与方差</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="14.511ex" height="6.354ex" role="img" focusable="false" viewBox="0 -1562.5 6414.1 2808.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-1-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D707" xlink:href="#MJX-1-TEX-I-1D707"></use></g><g data-mml-node="mi" transform="translate(636,-150) scale(0.707)"><use data-c="1D6FD" xlink:href="#MJX-1-TEX-I-1D6FD"></use></g></g><g data-mml-node="mo" transform="translate(1364,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2419.8,0)"><g data-mml-node="mn" transform="translate(409,676)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220,-686)"><use data-c="1D45A" xlink:href="#MJX-1-TEX-I-1D45A"></use></g><rect width="1078" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(3904.4,0)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-1-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(411.6,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45A" xlink:href="#MJX-1-TEX-I-1D45A"></use></g></g></g><g data-mml-node="msub" transform="translate(5515.1,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></g></svg></mjx-container><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="22.41ex" height="6.354ex" role="img" focusable="false" viewBox="0 -1562.5 9905.3 2808.5" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D70E" xlink:href="#MJX-1-TEX-I-1D70E"></use></g><g data-mml-node="mn" transform="translate(604,413) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(604,-265.5) scale(0.707)"><use data-c="1D6FD" xlink:href="#MJX-1-TEX-I-1D6FD"></use></g></g><g data-mml-node="mo" transform="translate(1332,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2387.8,0)"><g data-mml-node="mn" transform="translate(409,676)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220,-686)"><use data-c="1D45A" xlink:href="#MJX-1-TEX-I-1D45A"></use></g><rect width="1078" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(3872.4,0)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-1-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(411.6,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45A" xlink:href="#MJX-1-TEX-I-1D45A"></use></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5483.1,0)"><g data-mml-node="mo"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(1510.2,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="msub" transform="translate(2510.4,0)"><g data-mml-node="mi"><use data-c="1D707" xlink:href="#MJX-1-TEX-I-1D707"></use></g><g data-mml-node="TeXAtom" transform="translate(636,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D6FD" xlink:href="#MJX-1-TEX-I-1D6FD"></use></g></g></g></g><g data-mml-node="msup" transform="translate(9079.7,0)"><g data-mml-node="mo"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mn" transform="translate(422,413) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g></g></g></g></svg></mjx-container><p>②使用得到的均值与方差对数据进行归一化处理</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -4.118ex;" xmlns="http://www.w3.org/2000/svg" width="14.423ex" height="7.127ex" role="img" focusable="false" viewBox="0 -1330.2 6375.2 3150.2" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-1-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-1-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-1-TEX-LO-221A" d="M1001 1150Q1017 1150 1020 1132Q1020 1127 741 244L460 -643Q453 -650 436 -650H424Q423 -647 423 -645T421 -640T419 -631T415 -617T408 -594T399 -560T385 -512T367 -448T343 -364T312 -259L203 119L138 41L111 67L212 188L264 248L472 -474L983 1140Q988 1150 1001 1150Z"></path><path id="MJX-1-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-1-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-I-1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(449.5,16) translate(-250 0)"><use data-c="5E" xlink:href="#MJX-1-TEX-N-5E"></use></g></g></g><g data-mml-node="mo" transform="translate(1176.7,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2232.5,0)"><g data-mml-node="mrow" transform="translate(467.5,747.2)"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(1121.2,0)"><use data-c="2212" xlink:href="#MJX-1-TEX-N-2212"></use></g><g data-mml-node="msub" transform="translate(2121.4,0)"><g data-mml-node="mi"><use data-c="1D707" xlink:href="#MJX-1-TEX-I-1D707"></use></g><g data-mml-node="TeXAtom" transform="translate(636,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D6FD" xlink:href="#MJX-1-TEX-I-1D6FD"></use></g></g></g></g><g data-mml-node="msqrt" transform="translate(220,-1168.7)"><g transform="translate(1020,0)"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D70E" xlink:href="#MJX-1-TEX-I-1D70E"></use></g><g data-mml-node="mn" transform="translate(604,353.6) scale(0.707)"><use data-c="32" xlink:href="#MJX-1-TEX-N-32"></use></g><g data-mml-node="TeXAtom" transform="translate(604,-324.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D6FD" xlink:href="#MJX-1-TEX-I-1D6FD"></use></g></g></g><g data-mml-node="mo" transform="translate(1276.4,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(2276.7,0)"><use data-c="1D716" xlink:href="#MJX-1-TEX-I-1D716"></use></g></g><g data-mml-node="mo" transform="translate(0,-1.3)"><use data-c="221A" xlink:href="#MJX-1-TEX-LO-221A"></use></g><rect width="2682.7" height="60" x="1020" y="1088.7"></rect></g><rect width="3902.7" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container><p>③使用学习到的权重和偏置对归一化后的数据进行处理</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.685ex;" xmlns="http://www.w3.org/2000/svg" width="24.926ex" height="2.518ex" role="img" focusable="false" viewBox="0 -810 11017.2 1112.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-I-1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path><path id="MJX-1-TEX-N-2261" d="M56 444Q56 457 70 464H707Q722 456 722 444Q722 430 706 424H72Q56 429 56 444ZM56 237T56 250T70 270H707Q722 262 722 250T707 230H70Q56 237 56 250ZM56 56Q56 71 72 76H706Q722 70 722 56Q722 44 707 36H70Q56 43 56 56Z"></path><path id="MJX-1-TEX-I-1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path id="MJX-1-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-1-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-1-TEX-I-1D466"></use></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(1094.7,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(2150.5,0)"><use data-c="1D6FE" xlink:href="#MJX-1-TEX-I-1D6FE"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2693.5,0)"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(449.5,16) translate(-250 0)"><use data-c="5E" xlink:href="#MJX-1-TEX-N-5E"></use></g></g></g><g data-mml-node="mo" transform="translate(3814.7,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(4814.9,0)"><use data-c="1D6FD" xlink:href="#MJX-1-TEX-I-1D6FD"></use></g><g data-mml-node="mo" transform="translate(5658.7,0)"><use data-c="2261" xlink:href="#MJX-1-TEX-N-2261"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6714.5,0)"><g data-mml-node="mi"><use data-c="1D435" xlink:href="#MJX-1-TEX-I-1D435"></use></g><g data-mml-node="msub" transform="translate(759,0)"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-1-TEX-I-1D441"></use></g><g data-mml-node="TeXAtom" transform="translate(836,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D6FE" xlink:href="#MJX-1-TEX-I-1D6FE"></use></g><g data-mml-node="mo" transform="translate(543,0)"><use data-c="2C" xlink:href="#MJX-1-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(821,0)"><use data-c="1D6FD" xlink:href="#MJX-1-TEX-I-1D6FD"></use></g></g></g><g data-mml-node="mo" transform="translate(2625.8,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(3014.8,0)"><g data-mml-node="mi"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g><g data-mml-node="mo" transform="translate(3913.7,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></g></svg></mjx-container><p>&ensp;&ensp;&ensp;&ensp;值得注意的是，在训练阶段，每进入一批数据，都要计算这批数据的均值与权重；但在推理&#x2F;测试阶段，输入数据的批量都是1，因此在推理&#x2F;测试阶段，对数据进行归一化的均值和方差是在<strong>训练阶段</strong>平均各训练批量的均值和方差得到的。</p><p>&ensp;&ensp;&ensp;&ensp;对于图像来说，BN的操作如下图所示。</p><img src="BN.png" style="zoom: 50%;" /><h1 id="LayerNorm的步骤"><a href="#LayerNorm的步骤" class="headerlink" title="LayerNorm的步骤"></a>LayerNorm的步骤</h1><p>&ensp;&ensp;&ensp;&ensp;BatchNorm在输入数据批量数较小时，效果一般；即BatchNorm的计算依赖于mini-batch的size大小。LayerNorm很好的解决了这个问题，LayerNorm的步骤如下所示。</p><p>①计算出单个输入所有hidden units的均值与方差</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.819ex;" xmlns="http://www.w3.org/2000/svg" width="14.008ex" height="6.74ex" role="img" focusable="false" viewBox="0 -1733 6191.6 2978.9" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path id="MJX-1-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-1-TEX-I-1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-1-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path id="MJX-1-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D707" xlink:href="#MJX-1-TEX-I-1D707"></use></g><g data-mml-node="mi" transform="translate(636,413) scale(0.707)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g></g><g data-mml-node="mo" transform="translate(1174.5,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2230.3,0)"><g data-mml-node="mn" transform="translate(414,676)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220,-686)"><use data-c="1D43B" xlink:href="#MJX-1-TEX-I-1D43B"></use></g><rect width="1088" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(3724.9,0)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-1-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-1-TEX-N-31"></use></g></g><g data-mml-node="mi" transform="translate(408,1150) scale(0.707)"><use data-c="1D43B" xlink:href="#MJX-1-TEX-I-1D43B"></use></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5335.6,0)"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-1-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(562,413) scale(0.707)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(562,-247) scale(0.707)"><use data-c="1D456" xlink:href="#MJX-1-TEX-I-1D456"></use></g></g></g></g></g></svg></mjx-container><p>&ensp;&ensp;&ensp;&ensp;在公式中，H表示的是layer中hidden units的数量，可以看出，对于LN，同一layer下的所有hidden units共享相同的normalization项，但是不同的训练实例有不同的normalization项；LN不同于BN，它不会引入任何受限于mini-Batch的size大小的限制。</p><img src="LN.png" style="zoom: 67%;" /><p>&ensp;&ensp;&ensp;&ensp;下图是一个对于图像来说，更加直观的BN与LN之间的区别</p><img src="BN&LN.png" style="zoom:50%;" /><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&ensp;&ensp;&ensp;&ensp;总的来说，BatchNorm是依赖于batchSize的；LayerNorm不依赖于batchSize；在<strong>CV</strong>领域，使用BN层，传入的参数是通道数C，表示对特征图的每一个通道都进行归一化，归一化的均值与方差取自该通道的所有batch；使用LN层，一般传入的参数是[C, H, W]；表示对一整张图像都进行归一化处理，归一化的均值与方差仅取自这一张图像。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      <category domain="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97/">神经网络模块</category>
      
      
      <comments>http://example.com/2024/04/17/BatchNorm%E5%92%8CLayerNorm%E7%9A%84%E7%90%86%E8%A7%A3/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>YOLOv1论文研读</title>
      <link>http://example.com/2024/04/15/YOLOv1%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</link>
      <guid>http://example.com/2024/04/15/YOLOv1%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</guid>
      <pubDate>Mon, 15 Apr 2024 12:24:16 GMT</pubDate>
      
      
      
      
      
      
      <comments>http://example.com/2024/04/15/YOLOv1%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>CBAM论文研读</title>
      <link>http://example.com/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</link>
      <guid>http://example.com/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</guid>
      <pubDate>Sun, 14 Apr 2024 07:48:57 GMT</pubDate>
      
      <description>&lt;p&gt;本篇博客主要讲一下对CBAM模块的学习与了解，博客最后有Pytorch代码的实现~&lt;/p&gt;
&lt;p&gt;CBAM论文：&lt;a href=&quot;https://arxiv.org/abs/1807.06521&quot;&gt;https://arxiv.org/abs/1807.06521&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码地址：&lt;a href=&quot;https://github.com/Jongchan/attention-module&quot;&gt;https://github.com/Jongchan/attention-module&lt;/a&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>本篇博客主要讲一下对CBAM模块的学习与了解，博客最后有Pytorch代码的实现~</p><p>CBAM论文：<a href="https://arxiv.org/abs/1807.06521">https://arxiv.org/abs/1807.06521</a></p><p>代码地址：<a href="https://github.com/Jongchan/attention-module">https://github.com/Jongchan/attention-module</a></p> <span id="more"></span><h1 id="CBAM网络"><a href="#CBAM网络" class="headerlink" title="CBAM网络"></a>CBAM网络</h1><p>&ensp;&ensp;&ensp;&ensp;CBAM的全名是Convolutional Block Attention Module，用中文解释为卷积注意力模块，论文发布于2018年，CBAM可以理解为在SENet模块的基础上进行了改进，除了通道方向上进行了Attention操作，在空间维度上同样也进行了Attention操作。CBAM模块的网络结构如下图所示。</p><p><img src="/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/CBAM_module.png"></p><p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，特征图（Input Feature）分别经过通道注意力模块（Channel Attention Module，CAM）和空间注意力模块（Spatial Attention Module，SAM）得到了调整后的特征图（Refined Feature）。至于作者为何选用了这种处理顺序，在后面的消融实验中会给出结果。我们下面分别介绍CAM模块和SAM模块</p><h2 id="Channel-Attention-Module"><a href="#Channel-Attention-Module" class="headerlink" title="Channel Attention Module"></a>Channel Attention Module</h2><p>&ensp;&ensp;&ensp;&ensp;CAM模块的处理流程如下图所示。从下图可以看出，对于得到的特征图F，特征图大小为[H, W, C]，在其通道方向上分别做一个<strong>全局平均池化</strong>和<strong>全局最大池化</strong>，得到了两个1x1xC的特征图，将这两个特征图分别送入<strong>共享权重</strong>的全连接网络中。全连接网络共两层，第一层的神经元个数为C&#x2F;r（r为减少率），使用ReLU激活函数；第二层神经元个数为C。</p><p>&ensp;&ensp;&ensp;&ensp;两个1x1xC的特征图再经过全连接网络后，输出的大小依然是1x1xC；此时，将这两个特征进行一个逐元素相加（element-wise add）的操作，再经过sigmoid激活函数，生成最终的Channel Attention Feature（<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="3.075ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1359.2 840.8" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-2-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D440" xlink:href="#MJX-2-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(1003,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g></g></g></g></svg></mjx-container>)。</p><p><img src="/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/CBAM_CAM.png"></p><p>&ensp;&ensp;&ensp;&ensp;得到通道注意力特征后，将其与输入特征图F在通道方向上做一个逐元素相乘（element-wise multiple）操作，生成了Spatial attention module需要的输入特征。</p><p><img src="/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/CBAM_multipy.png"></p><p>&ensp;&ensp;&ensp;&ensp;具体的处理公式如下所示。</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.887ex;" xmlns="http://www.w3.org/2000/svg" width="92.113ex" height="2.584ex" role="img" focusable="false" viewBox="0 -750 40714.1 1142" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-2-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-2-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-2-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-2-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-2-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-2-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-2-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-2-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-2-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-2-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-2-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-2-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-2-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-2-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-2-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-2-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D440" xlink:href="#MJX-2-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(1003,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g></g><g data-mml-node="mo" transform="translate(1359.2,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1748.2,0)"><use data-c="1D439" xlink:href="#MJX-2-TEX-I-1D439"></use></g><g data-mml-node="mo" transform="translate(2497.2,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3164,0)"><use data-c="3D" xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(4219.7,0)"><use data-c="1D70E" xlink:href="#MJX-2-TEX-I-1D70E"></use></g><g data-mml-node="mo" transform="translate(4790.7,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(5179.7,0)"><use data-c="1D440" xlink:href="#MJX-2-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(6230.7,0)"><use data-c="1D43F" xlink:href="#MJX-2-TEX-I-1D43F"></use></g><g data-mml-node="mi" transform="translate(6911.7,0)"><use data-c="1D443" xlink:href="#MJX-2-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(7662.7,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(8051.7,0)"><use data-c="1D434" xlink:href="#MJX-2-TEX-I-1D434"></use></g><g data-mml-node="mi" transform="translate(8801.7,0)"><use data-c="1D463" xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(9286.7,0)"><use data-c="1D454" xlink:href="#MJX-2-TEX-I-1D454"></use></g><g data-mml-node="mi" transform="translate(9763.7,0)"><use data-c="1D443" xlink:href="#MJX-2-TEX-I-1D443"></use></g><g data-mml-node="mi" transform="translate(10514.7,0)"><use data-c="1D45C" xlink:href="#MJX-2-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(10999.7,0)"><use data-c="1D45C" xlink:href="#MJX-2-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(11484.7,0)"><use data-c="1D459" xlink:href="#MJX-2-TEX-I-1D459"></use></g><g data-mml-node="mo" transform="translate(11782.7,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(12171.7,0)"><use data-c="1D439" xlink:href="#MJX-2-TEX-I-1D439"></use></g><g data-mml-node="mo" transform="translate(12920.7,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(13309.7,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(13921,0)"><use data-c="2B" xlink:href="#MJX-2-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(14921.2,0)"><use data-c="1D440" xlink:href="#MJX-2-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(15972.2,0)"><use data-c="1D43F" xlink:href="#MJX-2-TEX-I-1D43F"></use></g><g data-mml-node="mi" transform="translate(16653.2,0)"><use data-c="1D443" xlink:href="#MJX-2-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(17404.2,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(17793.2,0)"><use data-c="1D440" xlink:href="#MJX-2-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(18844.2,0)"><use data-c="1D44E" xlink:href="#MJX-2-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(19373.2,0)"><use data-c="1D465" xlink:href="#MJX-2-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(19945.2,0)"><use data-c="1D443" xlink:href="#MJX-2-TEX-I-1D443"></use></g><g data-mml-node="mi" transform="translate(20696.2,0)"><use data-c="1D45C" xlink:href="#MJX-2-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(21181.2,0)"><use data-c="1D45C" xlink:href="#MJX-2-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(21666.2,0)"><use data-c="1D459" xlink:href="#MJX-2-TEX-I-1D459"></use></g><g data-mml-node="mo" transform="translate(21964.2,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(22353.2,0)"><use data-c="1D439" xlink:href="#MJX-2-TEX-I-1D439"></use></g><g data-mml-node="mo" transform="translate(23102.2,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(23491.2,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(23880.2,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(24547,0)"><use data-c="3D" xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(25602.7,0)"><use data-c="1D70E" xlink:href="#MJX-2-TEX-I-1D70E"></use></g><g data-mml-node="mo" transform="translate(26173.7,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(26562.7,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-2-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(27943.3,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(28332.3,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-2-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="30" xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mo" transform="translate(29712.8,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="msubsup" transform="translate(30101.8,0)"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-2-TEX-I-1D439"></use></g><g data-mml-node="mi" transform="translate(837.3,413) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g><g data-mml-node="TeXAtom" transform="translate(676,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-2-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(529,0)"><use data-c="1D463" xlink:href="#MJX-2-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(1014,0)"><use data-c="1D454" xlink:href="#MJX-2-TEX-I-1D454"></use></g></g></g><g data-mml-node="mo" transform="translate(31882.1,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(32271.1,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(32882.4,0)"><use data-c="2B" xlink:href="#MJX-2-TEX-N-2B"></use></g><g data-mml-node="msub" transform="translate(33882.6,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-2-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-2-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(35263.1,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(35652.1,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-2-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="30" xlink:href="#MJX-2-TEX-N-30"></use></g></g><g data-mml-node="mo" transform="translate(37032.7,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="msubsup" transform="translate(37421.7,0)"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-2-TEX-I-1D439"></use></g><g data-mml-node="mi" transform="translate(837.3,413) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g><g data-mml-node="TeXAtom" transform="translate(676,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45A" xlink:href="#MJX-2-TEX-I-1D45A"></use></g><g data-mml-node="mi" transform="translate(878,0)"><use data-c="1D44E" xlink:href="#MJX-2-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(1407,0)"><use data-c="1D465" xlink:href="#MJX-2-TEX-I-1D465"></use></g></g></g><g data-mml-node="mo" transform="translate(39547.1,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(39936.1,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(40325.1,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g></g></g></svg></mjx-container><p>&ensp;&ensp;&ensp;&ensp;上述公式中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewBox="0 -431 571 442" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D70E" xlink:href="#MJX-2-TEX-I-1D70E"></use></g></g></g></svg></mjx-container>表示sigmoid激活函数；<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.92ex" role="img" focusable="false" viewBox="0 -683 1380.6 848.6" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-2-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-2-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="30" xlink:href="#MJX-2-TEX-N-30"></use></g></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1380.6 833" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-2-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-2-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-2-TEX-N-31"></use></g></g></g></g></svg></mjx-container>表示两层全连接网络的权重参数，两个1x1xC的特征经过的是共享权重的全连接网络。</p><p>&ensp;&ensp;&ensp;&ensp;在实际操作过程中，对1x1xC的向量经过两次全连接网络，第一次全连接将向量压缩为原来的1&#x2F;16倍，特征长度变为1x1x（C&#x2F;16），使用ReLU激活函数；第二次全连接恢复到C，使用sigmoid激活函数。</p><p>&ensp;&ensp;&ensp;&ensp;将CAM模块与我们上一次讲到的SENet模块的区别在于，SENet只对特征图在通道方向上做了全局平均池化操作，而CAM模块分别作了全局平均池化与全局最大池化，并且将这两个特征图通过一个共享参数的全连接网络中，之后再把它们逐元素相加来得到通道注意力特征。</p><h2 id="Spatial-Attention-Module"><a href="#Spatial-Attention-Module" class="headerlink" title="Spatial Attention Module"></a>Spatial Attention Module</h2><p>&ensp;&ensp;&ensp;&ensp;SAM模块的处理过程如下图所示。</p><p><img src="/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/CBAM_SAM.png"></p><p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，SAM模块将CAM模块输出的特征图作为输入特征图<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.451ex" height="2.035ex" role="img" focusable="false" viewBox="0 -899.7 1083.5 899.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-2-TEX-V-2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-2-TEX-I-1D439"></use></g><g data-mml-node="TeXAtom" transform="translate(837.3,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(33,363) scale(0.707)"><use data-c="2032" xlink:href="#MJX-2-TEX-V-2032"></use></g></g></g></g></g></g></svg></mjx-container>；首先做一个基于channel的<strong>全局平均池化</strong>与*<em>全局最大池化</em>，得到两个HxWx1的特征图。将这两个特征图在通道方向上进行拼接，变为HxWx2的特征图。随后经过一个7x7的卷积（论文作者指出7x7卷积效果优于3x3的卷积），得到一个HxWx1的特征图，经过sigmoid激活函数后生成spatial attention feature（<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex;" xmlns="http://www.w3.org/2000/svg" width="3.133ex" height="1.901ex" role="img" focusable="false" viewBox="0 -683 1384.6 840.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-2-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D440" xlink:href="#MJX-2-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(1003,-150) scale(0.707)"><use data-c="1D460" xlink:href="#MJX-2-TEX-I-1D460"></use></g></g></g></g></svg></mjx-container>）。</p><p>&ensp;&ensp;&ensp;&ensp;将<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex;" xmlns="http://www.w3.org/2000/svg" width="3.133ex" height="1.901ex" role="img" focusable="false" viewBox="0 -683 1384.6 840.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D440" xlink:href="#MJX-1-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(1003,-150) scale(0.707)"><use data-c="1D460" xlink:href="#MJX-1-TEX-I-1D460"></use></g></g></g></g></svg></mjx-container>与特征图<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.451ex" height="2.035ex" role="img" focusable="false" viewBox="0 -899.7 1083.5 899.7" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-1-TEX-V-2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-1-TEX-I-1D439"></use></g><g data-mml-node="TeXAtom" transform="translate(837.3,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(33,363) scale(0.707)"><use data-c="2032" xlink:href="#MJX-1-TEX-V-2032"></use></g></g></g></g></g></g></svg></mjx-container>逐元素相乘，得到了最终的特征图。</p><img src="CBAM_final_feature.png" style="zoom:67%;" /><p>&ensp;&ensp;&ensp;&ensp;具体的处理公式如下所示。</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.887ex;" xmlns="http://www.w3.org/2000/svg" width="69.191ex" height="2.903ex" role="img" focusable="false" viewBox="0 -891 30582.2 1283" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path id="MJX-1-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-1-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-1-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-1-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-1-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path id="MJX-1-TEX-I-1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path><path id="MJX-1-TEX-I-1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path><path id="MJX-1-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-1-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-1-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-1-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-1-TEX-N-3B" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z"></path><path id="MJX-1-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-1-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-1-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path><path id="MJX-1-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D440" xlink:href="#MJX-1-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(1003,-150) scale(0.707)"><use data-c="1D460" xlink:href="#MJX-1-TEX-I-1D460"></use></g></g><g data-mml-node="mo" transform="translate(1384.6,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1773.6,0)"><use data-c="1D439" xlink:href="#MJX-1-TEX-I-1D439"></use></g><g data-mml-node="mo" transform="translate(2522.6,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3189.4,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(4245.2,0)"><use data-c="1D70E" xlink:href="#MJX-1-TEX-I-1D70E"></use></g><g data-mml-node="mo" transform="translate(4816.2,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(5205.2,0)"><g data-mml-node="mi"><use data-c="1D453" xlink:href="#MJX-1-TEX-I-1D453"></use></g><g data-mml-node="TeXAtom" transform="translate(636,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="37" xlink:href="#MJX-1-TEX-N-37"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="D7" xlink:href="#MJX-1-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278,0)"><use data-c="37" xlink:href="#MJX-1-TEX-N-37"></use></g></g></g><g data-mml-node="mo" transform="translate(7148.4,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(7537.4,0)"><use data-c="5B" xlink:href="#MJX-1-TEX-N-5B"></use></g><g data-mml-node="mi" transform="translate(7815.4,0)"><use data-c="1D434" xlink:href="#MJX-1-TEX-I-1D434"></use></g><g data-mml-node="mi" transform="translate(8565.4,0)"><use data-c="1D463" xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(9050.4,0)"><use data-c="1D454" xlink:href="#MJX-1-TEX-I-1D454"></use></g><g data-mml-node="mi" transform="translate(9527.4,0)"><use data-c="1D443" xlink:href="#MJX-1-TEX-I-1D443"></use></g><g data-mml-node="mi" transform="translate(10278.4,0)"><use data-c="1D45C" xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(10763.4,0)"><use data-c="1D45C" xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(11248.4,0)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g><g data-mml-node="mo" transform="translate(11546.4,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(11935.4,0)"><use data-c="1D439" xlink:href="#MJX-1-TEX-I-1D439"></use></g><g data-mml-node="mo" transform="translate(12684.4,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(13073.4,0)"><use data-c="3B" xlink:href="#MJX-1-TEX-N-3B"></use></g><g data-mml-node="mi" transform="translate(13518.1,0)"><use data-c="1D440" xlink:href="#MJX-1-TEX-I-1D440"></use></g><g data-mml-node="mi" transform="translate(14569.1,0)"><use data-c="1D44E" xlink:href="#MJX-1-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(15098.1,0)"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g><g data-mml-node="mi" transform="translate(15670.1,0)"><use data-c="1D443" xlink:href="#MJX-1-TEX-I-1D443"></use></g><g data-mml-node="mi" transform="translate(16421.1,0)"><use data-c="1D45C" xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(16906.1,0)"><use data-c="1D45C" xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(17391.1,0)"><use data-c="1D459" xlink:href="#MJX-1-TEX-I-1D459"></use></g><g data-mml-node="mo" transform="translate(17689.1,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(18078.1,0)"><use data-c="1D439" xlink:href="#MJX-1-TEX-I-1D439"></use></g><g data-mml-node="mo" transform="translate(18827.1,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(19216.1,0)"><use data-c="5D" xlink:href="#MJX-1-TEX-N-5D"></use></g><g data-mml-node="mo" transform="translate(19494.1,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(19883.1,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mspace" transform="translate(20272.1,0)"></g><g data-mml-node="mo" transform="translate(20549.9,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(21605.6,0)"><use data-c="1D70E" xlink:href="#MJX-1-TEX-I-1D70E"></use></g><g data-mml-node="mo" transform="translate(22176.6,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(22565.6,0)"><g data-mml-node="mi"><use data-c="1D453" xlink:href="#MJX-1-TEX-I-1D453"></use></g><g data-mml-node="TeXAtom" transform="translate(636,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="37" xlink:href="#MJX-1-TEX-N-37"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="D7" xlink:href="#MJX-1-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278,0)"><use data-c="37" xlink:href="#MJX-1-TEX-N-37"></use></g></g></g><g data-mml-node="mo" transform="translate(24508.9,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mo" transform="translate(24897.9,0)"><use data-c="5B" xlink:href="#MJX-1-TEX-N-5B"></use></g><g data-mml-node="msubsup" transform="translate(25175.9,0)"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-1-TEX-I-1D439"></use></g><g data-mml-node="mi" transform="translate(837.3,413) scale(0.707)"><use data-c="1D460" xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="TeXAtom" transform="translate(676,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-1-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(529,0)"><use data-c="1D463" xlink:href="#MJX-1-TEX-I-1D463"></use></g><g data-mml-node="mi" transform="translate(1014,0)"><use data-c="1D454" xlink:href="#MJX-1-TEX-I-1D454"></use></g></g></g><g data-mml-node="mo" transform="translate(26956.2,0)"><use data-c="3B" xlink:href="#MJX-1-TEX-N-3B"></use></g><g data-mml-node="msubsup" transform="translate(27400.8,0)"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-1-TEX-I-1D439"></use></g><g data-mml-node="mi" transform="translate(837.3,413) scale(0.707)"><use data-c="1D460" xlink:href="#MJX-1-TEX-I-1D460"></use></g><g data-mml-node="TeXAtom" transform="translate(676,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45A" xlink:href="#MJX-1-TEX-I-1D45A"></use></g><g data-mml-node="mi" transform="translate(878,0)"><use data-c="1D44E" xlink:href="#MJX-1-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(1407,0)"><use data-c="1D465" xlink:href="#MJX-1-TEX-I-1D465"></use></g></g></g><g data-mml-node="mo" transform="translate(29526.2,0)"><use data-c="5D" xlink:href="#MJX-1-TEX-N-5D"></use></g><g data-mml-node="mo" transform="translate(29804.2,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(30193.2,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg></mjx-container><p>&ensp;&ensp;&ensp;&ensp;上公式中<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="4.396ex" height="2.367ex" role="img" focusable="false" viewBox="0 -841 1943.2 1046" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-1-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-1-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D453" xlink:href="#MJX-1-TEX-I-1D453"></use></g><g data-mml-node="TeXAtom" transform="translate(636,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="37" xlink:href="#MJX-1-TEX-N-37"></use></g><g data-mml-node="mo" transform="translate(500,0)"><use data-c="D7" xlink:href="#MJX-1-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(1278,0)"><use data-c="37" xlink:href="#MJX-1-TEX-N-37"></use></g></g></g></g></g></svg></mjx-container>表示7x7的卷积；<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewBox="0 -431 571 442" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-1-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D70E" xlink:href="#MJX-1-TEX-I-1D70E"></use></g></g></g></svg></mjx-container>表示sigmoid激活函数。</p><p>&ensp;&ensp;&ensp;&ensp;以上就是CBAM模块的具体处理流程了~</p><h1 id="CBAM-ResBlock"><a href="#CBAM-ResBlock" class="headerlink" title="CBAM-ResBlock"></a>CBAM-ResBlock</h1><p>&ensp;&ensp;&ensp;&ensp;论文中，作者提出了一种CBAM模块与ResBlock相结合的网络模型，如下图所示。</p><p><img src="/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/CBAM_ResBlock.png"></p><p>&ensp;&ensp;&ensp;&ensp;具体做法就是在ResBlock模块中保留residual一侧，对于需要做Conv操作的一侧加上CBAM模块即可。</p><h1 id="实验（部分）"><a href="#实验（部分）" class="headerlink" title="实验（部分）"></a>实验（部分）</h1><p>&ensp;&ensp;&ensp;&ensp;以下是作者做的一些相关的实验，我们挑部分来讲。</p><h2 id="CAM模块实验"><a href="#CAM模块实验" class="headerlink" title="CAM模块实验"></a>CAM模块实验</h2><p>&ensp;&ensp;&ensp;&ensp;作者在ResNet50上嵌入了CAM模块，其中CAM模块使用了不同的特征聚合方法。</p><p><img src="/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/CBAM_Ex_1.png"></p><p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，在CAM模块中使用AvgPool+MaxPool的方式聚合特征，模型的效果最好。</p><h2 id="SAM模块实验"><a href="#SAM模块实验" class="headerlink" title="SAM模块实验"></a>SAM模块实验</h2><p>&ensp;&ensp;&ensp;&ensp;作者在上一步实验的基础上添加了SAM模块，实验了不同的卷积核大小与特征聚合方式。</p><p><img src="/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/CBAM_Ex_2.png"></p><p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，在SAM模块中使用7x7的大小的卷积核、特征聚合方式使用AvgPool+MaxPool可以获得最好的效果。</p><h2 id="CBAM实验"><a href="#CBAM实验" class="headerlink" title="CBAM实验"></a>CBAM实验</h2><p>&ensp;&ensp;&ensp;&ensp;作者在此实验中探讨了如何确定SAM和CAM模块的处理顺序。</p><p><img src="/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/CBAM_Ex_3.png"></p><p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，特征图先经过CAM模块，再经过SAM模块，能够获得最好的效果。</p><h1 id="代码（Pytorch）"><a href="#代码（Pytorch）" class="headerlink" title="代码（Pytorch）"></a>代码（Pytorch）</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import math</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torchinfo import summary</span><br><span class="line"></span><br><span class="line">def autopad(k, p=None):  # kernel, padding</span><br><span class="line">    # Pad to &#x27;same&#x27;</span><br><span class="line">    if p is None:</span><br><span class="line">        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad</span><br><span class="line">    return p</span><br><span class="line"># Conv + BN + ReLU</span><br><span class="line">class BasicConv(nn.Module):</span><br><span class="line">    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param in_planes: 输入通道数</span><br><span class="line">        :param out_planes: 输出通道数</span><br><span class="line">        :param kernel_size: 卷积核大小</span><br><span class="line">        :param stride: 步长</span><br><span class="line">        :param padding: 填充</span><br><span class="line">        :param dilation: 空洞卷积</span><br><span class="line">        :param groups: 分组卷积</span><br><span class="line">        :param relu: 是否使用ReLU激活函数</span><br><span class="line">        :param bn: 是否使用BN层</span><br><span class="line">        :param bias: 是否全用偏置项</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super(BasicConv, self).__init__()</span><br><span class="line">        self.out_channels = out_planes</span><br><span class="line">        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)</span><br><span class="line">        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None</span><br><span class="line">        self.relu = nn.ReLU() if relu else None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        if self.bn is not None:</span><br><span class="line">            x = self.bn(x)</span><br><span class="line">        if self.relu is not None:</span><br><span class="line">            x = self.relu(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">class Flatten(nn.Module):</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return x.view(x.size(0), -1)</span><br><span class="line"></span><br><span class="line"># CAM 通道注意力</span><br><span class="line">class ChannelGate(nn.Module):</span><br><span class="line">    def __init__(self, gate_channels, reduction_ratio=16, pool_types=[&#x27;avg&#x27;, &#x27;max&#x27;]):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param gate_channels: 输入的个数(输入通道数)</span><br><span class="line">        :param reduction_ratio: 第一层神经元下采样倍数</span><br><span class="line">        :param pool_types: 对特征图采用什么池化方式得到 1x1xC的特征</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super(ChannelGate, self).__init__()</span><br><span class="line">        self.gate_channels = gate_channels</span><br><span class="line">        self.mlp = nn.Sequential(</span><br><span class="line">            Flatten(),</span><br><span class="line">            nn.Linear(gate_channels, gate_channels // reduction_ratio),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(gate_channels // reduction_ratio, gate_channels)</span><br><span class="line">            )</span><br><span class="line">        self.pool_types = pool_types</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        channel_att_sum = None</span><br><span class="line">        for pool_type in self.pool_types:</span><br><span class="line">            if pool_type == &#x27;avg&#x27;:</span><br><span class="line">                avg_pool = F.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))</span><br><span class="line">                channel_att_raw = self.mlp(avg_pool)</span><br><span class="line">            elif pool_type == &#x27;max&#x27;:</span><br><span class="line">                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))</span><br><span class="line">                channel_att_raw = self.mlp(max_pool)</span><br><span class="line">            elif pool_type == &#x27;lp&#x27;:</span><br><span class="line">                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))</span><br><span class="line">                channel_att_raw = self.mlp(lp_pool)</span><br><span class="line">            elif pool_type == &#x27;lse&#x27;:</span><br><span class="line">                # LSE pool only</span><br><span class="line">                lse_pool = logsumexp_2d(x)</span><br><span class="line">                channel_att_raw = self.mlp(lse_pool)</span><br><span class="line"></span><br><span class="line">            if channel_att_sum is None:</span><br><span class="line">                channel_att_sum = channel_att_raw</span><br><span class="line">            else:</span><br><span class="line">                channel_att_sum = channel_att_sum + channel_att_raw</span><br><span class="line"></span><br><span class="line">        scale = F.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)</span><br><span class="line">        return x * scale</span><br><span class="line"></span><br><span class="line"># 对特征图在 H和W方向上做 logSumExp 池化 得到 1x1xC 的特征向量</span><br><span class="line">def logsumexp_2d(tensor):</span><br><span class="line">    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)</span><br><span class="line">    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)</span><br><span class="line">    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()</span><br><span class="line">    return outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对 特征图 HxWxC 在 C方向上分别最最大池化和最小池化 然后进行拼接</span><br><span class="line">class ChannelPool(nn.Module):</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return torch.cat((torch.max(x, 1)[0].unsqueeze(1), torch.mean(x, 1).unsqueeze(1)), dim=1 )</span><br><span class="line"></span><br><span class="line"># SAM 空间注意力</span><br><span class="line">class SpatialGate(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(SpatialGate, self).__init__()</span><br><span class="line">        kernel_size = 7</span><br><span class="line">        self.compress = ChannelPool()</span><br><span class="line">        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x_compress = self.compress(x)</span><br><span class="line">        x_out = self.spatial(x_compress)</span><br><span class="line">        scale = F.sigmoid(x_out) # broadcasting</span><br><span class="line">        return x * scale</span><br><span class="line"></span><br><span class="line">class CBAM(nn.Module):</span><br><span class="line">    def __init__(self, gate_channels, reduction_ratio=16, pool_types=[&#x27;avg&#x27;, &#x27;max&#x27;], no_spatial=False):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param gate_channels: 输入通道数</span><br><span class="line">        :param reduction_ratio: 下采样倍数</span><br><span class="line">        :param pool_types: CAM过程中的</span><br><span class="line">        :param no_spatial: 是否要做通道卷积</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super(CBAM, self).__init__()</span><br><span class="line">        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)</span><br><span class="line">        self.no_spatial=no_spatial</span><br><span class="line">        if not no_spatial:</span><br><span class="line">            self.SpatialGate = SpatialGate()</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x_out = self.ChannelGate(x)</span><br><span class="line">        if not self.no_spatial:</span><br><span class="line">            x_out = self.SpatialGate(x_out)</span><br><span class="line">        return x_out</span><br><span class="line"></span><br><span class="line"># 带有skip-connect模块的CBAM</span><br><span class="line">class ResBlock_CBAM(nn.Module):</span><br><span class="line">    def __init__(self, gate_channels, reduction_ratio=16, pool_types=[&#x27;avg&#x27;, &#x27;max&#x27;], no_spatial=False):</span><br><span class="line">        super(ResBlock_CBAM, self).__init__()</span><br><span class="line">        self.Conv = BasicConv(gate_channels, gate_channels, kernel_size=3, padding=autopad(3))</span><br><span class="line">        self.ChannelGate = ChannelGate(gate_channels)</span><br><span class="line">        self.no_spatial=no_spatial</span><br><span class="line">        if not no_spatial:</span><br><span class="line">            self.SpatialGate = SpatialGate()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        residual = x</span><br><span class="line">        out = self.Conv(x)</span><br><span class="line">        out = self.ChannelGate(out)</span><br><span class="line">        if not self.no_spatial:</span><br><span class="line">            out = self.SpatialGate(out)</span><br><span class="line">        return out + residual</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    x = torch.randn(1, 32, 16, 16)  # 创建随机输入张量</span><br><span class="line">    # model = ResBlock_CBAM(32)     # 创建 CBAM-ResBlock模型</span><br><span class="line">    model = CBAM(32)                # 创建 CBAM模型</span><br><span class="line">    print(model(x).shape)</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&ensp;&ensp;&ensp;&ensp;CBAM在SENet的基础上对通道注意力进行了改进，同时考虑了特征图在空间方向上的注意力，这两项改进使得CBAM在模型性能上优于SENet~</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      <category domain="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97/">神经网络模块</category>
      
      
      <comments>http://example.com/2024/04/14/CBAM%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>SENet论文研读</title>
      <link>http://example.com/2024/04/12/SENet%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</link>
      <guid>http://example.com/2024/04/12/SENet%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/</guid>
      <pubDate>Fri, 12 Apr 2024 09:19:07 GMT</pubDate>
      
      <description>&lt;p&gt;本篇博客主要讲一下鸽砸对SENet模块的解读，在最后会有PyTorch版本的代码~&lt;/p&gt;
&lt;p&gt;SENet论文网址：[&lt;a href=&quot;https://arxiv.org/abs/1709.01507&quot;&gt;1709.01507] Squeeze-and-Excitation Networks (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码地址：&lt;a href=&quot;https://github.com/hujie-frank/SENet&quot;&gt;hujie-frank&amp;#x2F;SENet: Squeeze-and-Excitation Networks (github.com)&lt;/a&gt;&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>本篇博客主要讲一下鸽砸对SENet模块的解读，在最后会有PyTorch版本的代码~</p><p>SENet论文网址：[<a href="https://arxiv.org/abs/1709.01507">1709.01507] Squeeze-and-Excitation Networks (arxiv.org)</a></p><p>代码地址：<a href="https://github.com/hujie-frank/SENet">hujie-frank&#x2F;SENet: Squeeze-and-Excitation Networks (github.com)</a></p> <span id="more"></span><h1 id="SENet网络"><a href="#SENet网络" class="headerlink" title="SENet网络"></a>SENet网络</h1><p>&ensp;&ensp;&ensp;&ensp;SENet论文全名《Squeeze-and-Excitation Networks》，用中文理解就是挤压与激励网络。SENet主要用在卷积神经网络里面，对某一层输出的特征图进行处理的一种网络模块。SENet主要的网络结构如下图所示。</p><p><img src="/2024/04/12/SENet%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SENet_model-1713077963171-4.png"></p><p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，一个大小为[C’,H’,W’]的特征图X首先经过处理，得到一个大小为[C,H,W]的特征图U。随后的过程就是SENet的处理流程，通过SENet之后，得到的是一个大小为[C,H,W]的特征图<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.928ex" height="2.292ex" role="img" focusable="false" viewBox="0 -1013 852 1013" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-3-TEX-SO-2DC" d="M374 597Q337 597 269 627T160 658Q101 658 34 606L24 597L12 611Q1 624 1 626Q1 627 27 648T55 671Q120 722 182 722Q219 722 286 692T395 661Q454 661 521 713L531 722L543 708Q554 695 554 693Q554 692 528 671T500 648Q434 597 374 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use data-c="1D44B" xlink:href="#MJX-3-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(515.3,191) translate(-278 0)"><use data-c="2DC" xlink:href="#MJX-3-TEX-SO-2DC"></use></g></g></g></g></g></svg></mjx-container>。因此，SENet可以保证模块前后的特征图大小不变，可以嵌入到现有的网络结构里面~</p><p>&ensp;&ensp;&ensp;&ensp;SENet的处理流程可以分为Squeeze（挤压）和Excitation（激励）两个步骤，在此基础上还有一个逐元素乘积Channel wise-multiplication。我们下面分别进行介绍</p><h2 id="Squeeze"><a href="#Squeeze" class="headerlink" title="Squeeze"></a>Squeeze</h2><p>&ensp;&ensp;&ensp;&ensp;Squeeze是对特征图U在通道方向（C）上做一个<strong>全局平均池化</strong>，得到一个1x1xC的向量。对应的部分如下图所示。</p><img src="SENet_squeeze-1713077976650-6.png" style="zoom:50%;" /><p>&ensp;&ensp;&ensp;&ensp;处理公式如下：</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.014ex;" xmlns="http://www.w3.org/2000/svg" width="37.49ex" height="6.935ex" role="img" focusable="false" viewBox="0 -1733 16570.7 3065.1" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-3-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-3-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-3-TEX-I-1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-I-1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-3-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path id="MJX-3-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-3-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-3-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-3-TEX-I-1D467"></use></g><g data-mml-node="mi" transform="translate(498,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-3-TEX-I-1D450"></use></g></g><g data-mml-node="mo" transform="translate(1132,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(2187.7,0)"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-3-TEX-I-1D439"></use></g><g data-mml-node="TeXAtom" transform="translate(676,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-3-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(469,0)"><use data-c="1D45E" xlink:href="#MJX-3-TEX-I-1D45E"></use></g></g></g><g data-mml-node="mo" transform="translate(3570.6,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(3959.6,0)"><g data-mml-node="mi"><use data-c="1D462" xlink:href="#MJX-3-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-3-TEX-I-1D450"></use></g></g><g data-mml-node="mo" transform="translate(4920.8,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(5587.6,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(6643.4,0)"><g data-mml-node="mn" transform="translate(1549.2,676)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mi"><use data-c="1D43B" xlink:href="#MJX-3-TEX-I-1D43B"></use></g><g data-mml-node="mo" transform="translate(1110.2,0)"><use data-c="D7" xlink:href="#MJX-3-TEX-N-D7"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2110.4,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g></g></g><rect width="3358.4" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(10408.5,0)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-3-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(148.2,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(408,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D43B" xlink:href="#MJX-3-TEX-I-1D43B"></use></g></g></g><g data-mml-node="munderover" transform="translate(12019.1,0)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-3-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(124.5,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D457" xlink:href="#MJX-3-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(412,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1190,0)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g></g><g data-mml-node="TeXAtom" transform="translate(351.5,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g></g></g><g data-mml-node="msub" transform="translate(13629.8,0)"><g data-mml-node="mi"><use data-c="1D462" xlink:href="#MJX-3-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-3-TEX-I-1D450"></use></g></g><g data-mml-node="mo" transform="translate(14591,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(14980,0)"><use data-c="1D456" xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(15325,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(15769.7,0)"><use data-c="1D457" xlink:href="#MJX-3-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(16181.7,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg></mjx-container><h2 id="Excitation"><a href="#Excitation" class="headerlink" title="Excitation"></a>Excitation</h2><p>&ensp;&ensp;&ensp;&ensp;Excitation主要是对上一步中得到的1x1xC大小的向量进行处理，处理流程是将这个向量依次经过两次<strong>全连接网络</strong>，第一次全连接将向量压缩为C&#x2F;16倍（我认为这个缩放的倍数可以根据自己的情况修改），使用ReLU激活函数；第二次全连接再恢复到C，使用sigmoid激活函数，最终得到的也是1x1xC大小的向量。对应的网络结构如下图所示。</p><img src="SENet_excitation.png" style="zoom:50%;" /><p>&ensp;&ensp;&ensp;&ensp;处理公式如下：</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="43.041ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 19024.1 1000" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-3-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-3-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-3-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-3-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-3-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path id="MJX-3-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-3-TEX-I-1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-3-TEX-I-1D460"></use></g><g data-mml-node="mo" transform="translate(746.8,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(1802.6,0)"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-3-TEX-I-1D439"></use></g><g data-mml-node="TeXAtom" transform="translate(676,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-3-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(466,0)"><use data-c="1D465" xlink:href="#MJX-3-TEX-I-1D465"></use></g></g></g><g data-mml-node="mo" transform="translate(3262.5,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(3651.5,0)"><use data-c="1D467" xlink:href="#MJX-3-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(4116.5,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(4561.2,0)"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g><g data-mml-node="mo" transform="translate(5609.2,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(6276,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(7331.8,0)"><use data-c="1D70E" xlink:href="#MJX-3-TEX-I-1D70E"></use></g><g data-mml-node="mo" transform="translate(7902.8,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(8291.8,0)"><use data-c="1D454" xlink:href="#MJX-3-TEX-I-1D454"></use></g><g data-mml-node="mo" transform="translate(8768.8,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(9157.8,0)"><use data-c="1D467" xlink:href="#MJX-3-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(9622.8,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(10067.4,0)"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g><g data-mml-node="mo" transform="translate(11115.4,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(11504.4,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(12171.2,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mi" transform="translate(13227,0)"><use data-c="1D70E" xlink:href="#MJX-3-TEX-I-1D70E"></use></g><g data-mml-node="mo" transform="translate(13798,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(14187,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-3-TEX-N-32"></use></g></g><g data-mml-node="mi" transform="translate(15567.5,0)"><use data-c="1D6FF" xlink:href="#MJX-3-TEX-I-1D6FF"></use></g><g data-mml-node="mo" transform="translate(16011.5,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(16400.5,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g></g><g data-mml-node="mi" transform="translate(17781.1,0)"><use data-c="1D467" xlink:href="#MJX-3-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(18246.1,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(18635.1,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg></mjx-container><p>&ensp;&ensp;&ensp;&ensp;在上公式中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.292ex" height="1ex" role="img" focusable="false" viewBox="0 -431 571 442" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D70E" xlink:href="#MJX-3-TEX-I-1D70E"></use></g></g></g></svg></mjx-container>表示ReLU激活函数；<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.005ex" height="1.645ex" role="img" focusable="false" viewBox="0 -717 444 727" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D6FF" xlink:href="#MJX-3-TEX-I-1D6FF"></use></g></g></g></svg></mjx-container>表示sigmoid激活函数；<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1380.6 833" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1380.6 833" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-3-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-3-TEX-N-32"></use></g></g></g></g></svg></mjx-container>表示的是全连接网络的权重参数。</p><h2 id="Channel-wise-multiplication"><a href="#Channel-wise-multiplication" class="headerlink" title="Channel-wise multiplication"></a>Channel-wise multiplication</h2><p>&ensp;&ensp;&ensp;&ensp;这一部分是将Excitation得到的1x1xC的向量与特征图X在通道方向上<strong>逐元素相乘</strong>，得到新的特征图<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.928ex" height="2.292ex" role="img" focusable="false" viewBox="0 -1013 852 1013" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-2-TEX-SO-2DC" d="M374 597Q337 597 269 627T160 658Q101 658 34 606L24 597L12 611Q1 624 1 626Q1 627 27 648T55 671Q120 722 182 722Q219 722 286 692T395 661Q454 661 521 713L531 722L543 708Q554 695 554 693Q554 692 528 671T500 648Q434 597 374 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use data-c="1D44B" xlink:href="#MJX-2-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(515.3,191) translate(-278 0)"><use data-c="2DC" xlink:href="#MJX-2-TEX-SO-2DC"></use></g></g></g></g></g></svg></mjx-container>。对应的网络结构如下图所示。</p><img src="SENet_channelwisemultiplication.png" style="zoom:50%;" /><p>&ensp;&ensp;&ensp;&ensp;处理的公式如下：</p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="26.575ex" height="2.857ex" role="img" focusable="false" viewBox="0 -1013 11746.2 1263" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-2-TEX-I-1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path id="MJX-2-TEX-SO-2DC" d="M374 597Q337 597 269 627T160 658Q101 658 34 606L24 597L12 611Q1 624 1 626Q1 627 27 648T55 671Q120 722 182 722Q219 722 286 692T395 661Q454 661 521 713L531 722L543 708Q554 695 554 693Q554 692 528 671T500 648Q434 597 374 597Z"></path><path id="MJX-2-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-2-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-2-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-2-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-2-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-2-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-2-TEX-I-1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-2-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-2-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><use data-c="1D44B" xlink:href="#MJX-2-TEX-I-1D44B"></use></g><g data-mml-node="mo" transform="translate(515.3,191) translate(-278 0)"><use data-c="2DC" xlink:href="#MJX-2-TEX-SO-2DC"></use></g></g></g><g data-mml-node="mi" transform="translate(861,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g></g><g data-mml-node="mo" transform="translate(1495,0)"><use data-c="3D" xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(2550.7,0)"><g data-mml-node="mi"><use data-c="1D439" xlink:href="#MJX-2-TEX-I-1D439"></use></g><g data-mml-node="TeXAtom" transform="translate(676,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-2-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(469,0)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g><g data-mml-node="mi" transform="translate(902,0)"><use data-c="1D44E" xlink:href="#MJX-2-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(1431,0)"><use data-c="1D459" xlink:href="#MJX-2-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(1729,0)"><use data-c="1D452" xlink:href="#MJX-2-TEX-I-1D452"></use></g></g></g><g data-mml-node="mo" transform="translate(4828.8,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="msub" transform="translate(5217.8,0)"><g data-mml-node="mi"><use data-c="1D462" xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g></g><g data-mml-node="mo" transform="translate(6179,0)"><use data-c="2C" xlink:href="#MJX-2-TEX-N-2C"></use></g><g data-mml-node="msub" transform="translate(6623.7,0)"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-2-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g></g><g data-mml-node="mo" transform="translate(7481.9,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(8148.6,0)"><use data-c="3D" xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="msub" transform="translate(9204.4,0)"><g data-mml-node="mi"><use data-c="1D460" xlink:href="#MJX-2-TEX-I-1D460"></use></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g></g><g data-mml-node="mo" transform="translate(10284.8,0)"><use data-c="22C5" xlink:href="#MJX-2-TEX-N-22C5"></use></g><g data-mml-node="msub" transform="translate(10785,0)"><g data-mml-node="mi"><use data-c="1D462" xlink:href="#MJX-2-TEX-I-1D462"></use></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><use data-c="1D450" xlink:href="#MJX-2-TEX-I-1D450"></use></g></g></g></g></svg></mjx-container><p>&ensp;&ensp;&ensp;&ensp;以上就是SENet模块的具体处理流程啦~</p><h1 id="SENet-Inception"><a href="#SENet-Inception" class="headerlink" title="SENet-Inception"></a>SENet-Inception</h1><p>&ensp;&ensp;&ensp;&ensp;在论文中，作者将SENet插入到了不同网络模块中，我们在这里简单看看作者画的图吧~</p><p>&ensp;&ensp;&ensp;&ensp;这个是将SENet嵌入到GoogleNet的Inception模块里面，在Inception模块的输出侧嵌入SENet。</p><img src="SENet_Inception.png" style="zoom:50%;" /><h1 id="SENet-ResNet"><a href="#SENet-ResNet" class="headerlink" title="SENet-ResNet"></a>SENet-ResNet</h1><p>&ensp;&ensp;&ensp;&ensp;这个是将SENet嵌入到ResNet的残差结构中，在ResNet需要通过卷积处理的分支嵌入一个SENet，这样处理后再和原本的输入residual做一个残差结构上的相加。</p><img src="SENet_ResNet.png" style="zoom:50%;" /><h1 id="实验（部分）"><a href="#实验（部分）" class="headerlink" title="实验（部分）"></a>实验（部分）</h1><p>&ensp;&ensp;&ensp;&ensp;论文作者最后开展了一些实验，用来验证SENet嵌入到现有网络里面，能够达到更好的效果。</p><p><img src="/2024/04/12/SENet%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/SENet_experiment.png"></p><p>&ensp;&ensp;&ensp;&ensp;从上图可以看出，作者在ResNet、ResNeXt、VGG、Inception等网络中嵌入了SENet，结果表明嵌入了SENet的网络能够达到更好的效果~</p><h1 id="代码（PyTorch版）"><a href="#代码（PyTorch版）" class="headerlink" title="代码（PyTorch版）"></a>代码（PyTorch版）</h1><p>&ensp;&ensp;&ensp;&ensp;以下是鸽砸使用PyTorch实现的SENet模块~</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">import torch  # 导入 PyTorch 库</span><br><span class="line">import torch.nn.functional as F  # 导入 PyTorch 的函数库</span><br><span class="line">import torch.nn as nn  # 导入 PyTorch 的神经网络模块</span><br><span class="line">from torchinfo import summary</span><br><span class="line"></span><br><span class="line">def autopad(k, p=None):  # kernel, padding</span><br><span class="line">    # Pad to &#x27;same&#x27;</span><br><span class="line">    if p is None:</span><br><span class="line">        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad</span><br><span class="line">    return p</span><br><span class="line"></span><br><span class="line"># 带有残差结构的SENet</span><br><span class="line">class SEResConv(nn.Module):</span><br><span class="line">    def __init__(self, c1, c2, kernel_size=1, stride=1):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param c1: 输入通道数</span><br><span class="line">        :param c2: 输出通道数</span><br><span class="line">        :param kernel_size: 卷积核大小</span><br><span class="line">        :param stride: 步长</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super(SEResConv, self).__init__()</span><br><span class="line">        assert c2 &gt;= 16  # 断言 输出通道数 大于等于 16(放缩倍数)</span><br><span class="line">        self.conv1 = nn.Conv2d(c1, c2, kernel_size, stride, padding=autopad(kernel_size), bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(c2)</span><br><span class="line">        self.act = nn.SiLU()</span><br><span class="line"></span><br><span class="line">        # 全连接层</span><br><span class="line">        self.fc1 = nn.Linear(in_features=c2, out_features=round(c2 / 16))</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=round(c2 / 16), out_features=c2)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        # Conv + BN + SiLU</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.act(out)</span><br><span class="line">        original_out = out</span><br><span class="line"></span><br><span class="line">        # 全局平均池化</span><br><span class="line">        out = nn.functional.avg_pool2d(out, kernel_size=out.shape[2], stride=1)</span><br><span class="line">        out = out.view(out.size(0), -1)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        out = self.sigmoid(out)</span><br><span class="line"></span><br><span class="line">        out = out.view(out.size(0), out.size(1), 1, 1)</span><br><span class="line">        out = out * original_out</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.act(out)</span><br><span class="line"></span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line"># 不带有残差结构的SENet</span><br><span class="line">class SEConv(nn.Module):</span><br><span class="line">    def __init__(self, c1, c2, kernel_size=1, stride=1):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param c1: 输入通道数</span><br><span class="line">        :param c2: 输出通道数</span><br><span class="line">        :param kernel_size: 卷积核大小</span><br><span class="line">        :param stride: 步长</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super(SEConv, self).__init__()</span><br><span class="line">        assert c2 &gt;= 16  # 断言 输出通道数 大于等于 16(放缩倍数)</span><br><span class="line">        self.conv1 = nn.Conv2d(c1, c2, kernel_size, stride, padding=autopad(kernel_size), bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(c2)</span><br><span class="line">        self.act = nn.SiLU()</span><br><span class="line"></span><br><span class="line">        # 全连接层</span><br><span class="line">        self.fc1 = nn.Linear(in_features=c2, out_features=round(c2 / 16))</span><br><span class="line">        self.relu = nn.ReLU(inplace=True)</span><br><span class="line">        self.fc2 = nn.Linear(in_features=round(c2 / 16), out_features=c2)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # Conv + BN + SiLU</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.act(out)</span><br><span class="line">        original_out = out</span><br><span class="line"></span><br><span class="line">        # 全局平均池化</span><br><span class="line">        out = nn.functional.avg_pool2d(out, kernel_size=out.shape[2], stride=1)</span><br><span class="line">        out = out.view(out.size(0), -1)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        out = self.sigmoid(out)</span><br><span class="line"></span><br><span class="line">        out = out.view(out.size(0), out.size(1), 1, 1)</span><br><span class="line">        out = out * original_out</span><br><span class="line"></span><br><span class="line">        out = self.act(out)</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    x = torch.randn(1, 32, 16, 16)  # 创建随机输入张量</span><br><span class="line">    # model = SEResConv(32, 32, 3)  # 创建 SENet-ResBlock 模型</span><br><span class="line">    model = SEConv(32, 32, 3)       # 创建 SENet-Conv模型</span><br><span class="line">    summary(model, input_size=(1, 32, 16, 16))</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>&ensp;&ensp;&ensp;&ensp;总的来说，SENet通过对特征图的通道（C）方向的全局池化、全连接层等处理，使得特征图在不同通道方向上的信息进行了交互，得到了类似于通道注意力的能力。</p><p>&ensp;&ensp;&ensp;&ensp;以上就是鸽砸对SENet模块的研读，有讲的不好的地方多多包涵，欢迎大家的讨论~</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</category>
      
      
      <category domain="http://example.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97/">神经网络模块</category>
      
      
      <comments>http://example.com/2024/04/12/SENet%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
